{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_project_copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinoanasta/IsiZulu-To-English/blob/main/IsiZulu-To-English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndE20vfiT_5U"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaVKiz8eDwuZ"
      },
      "source": [
        "import collections\n",
        "from collections import defaultdict\n",
        "\n",
        "import helper\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import skimage\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import img_as_float\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as ss\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "#from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "from tokenizers import Tokenizer as Toke\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K3nPQ22YHZl"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H2kLRBuDYVv",
        "outputId": "a3f446bf-813e-4983-e160-5eb57efd518b"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "C9b1mhbjDbIQ",
        "outputId": "d4e64f92-c94c-4b3e-db9f-3f4fb4488f59"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be3vAORiDk97",
        "outputId": "fbd8ea94-87d4-44c7-b8c8-0798c6605b3d"
      },
      "source": [
        "cd drive/MyDrive/NLP Project"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymARNmXpDpBy",
        "outputId": "7471f267-0627-47c6-8fe5-656411f40076"
      },
      "source": [
        "# read in images\n",
        "file_names = os.listdir()\n",
        "print(file_names)\n",
        "print(len(file_names))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['COMS4054A _ COMS7066A - NLP - Project.pdf', 'en-zu.training.csv', 'zu-en.eval.csv', 'zu-en.training.csv', 'en-zu.eval.csv', 'Data Statement for Umsuka isiZulu Parallel Corpus (1).pdf']\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40VfR7poDvZP",
        "outputId": "248762c1-94ae-475a-fcef-cdfad77ad636"
      },
      "source": [
        "df_train = pd.read_csv('en-zu.training.csv')\n",
        "print(df_train.head(3).columns)\n",
        "df_eval = pd.read_csv('en-zu.eval.csv')\n",
        "print(df_eval.head(3).columns)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['en', 'zu', 'source'], dtype='object')\n",
            "Index(['en', 'zu', 'zu.1', 'Source'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOo6aSh9EMvh",
        "outputId": "ced7c5d8-9ec8-400c-d6e2-0bb158b4d881"
      },
      "source": [
        "print(df_train['en'], df_train['zu'])\n",
        "english_sentences = df_train['en']\n",
        "zulu_sentences = df_train['zu']\n",
        "\n",
        "for i in range(len(english_sentences)):\n",
        "  english_sentences[i] = english_sentences[i].lower()\n",
        "  zulu_sentences[i] = zulu_sentences[i].lower()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0       His solo albums in the 60s were some of the mo...\n",
            "1       From which it follows that if you want to save...\n",
            "2       Thirty years after St. John Paul II begged Moz...\n",
            "3       Wynford yw sylfaenydd Stafell Fyw yng Nghaerdy...\n",
            "4       Unai Emery's side have won just two of their l...\n",
            "                              ...                        \n",
            "4734    The picture shows the infant breastfeeding whi...\n",
            "4735    Blame the producers, who constantly switch and...\n",
            "4736                              (Javier Garcia/BPI/REX)\n",
            "4737    \"I have enormous respect for Laura, She does a...\n",
            "4738    But despite the chaotic circumstances of the s...\n",
            "Name: en, Length: 4739, dtype: object 0       Ama-albhamu akhe e-solo eminyaka yawo-60 angam...\n",
            "1       Ukusuka lapho kuzobe sekulandela ukuthi uma uf...\n",
            "2       Ngemuva kweminyaka engamashumi amathathu u-St....\n",
            "3       Wynford yw sylfaenydd Stafell Fyw yng Nghaerdy...\n",
            "4       Iqembu lika-Unai Emery lisanqobe imidlalo emib...\n",
            "                              ...                        \n",
            "4734    Isithombe sibonisa ingane encane encelayo nges...\n",
            "4735    Sola abakhiqizi, abaguqula njalo babeke endawe...\n",
            "4736                        (U-Javier Garcia/i-BPI/i-REX)\n",
            "4737    \"Ngimhlonipha kakhulu u- Laura, Wenza umsebenz...\n",
            "4738    Kodwa nanoma bekunezimo zokuvalwa ezibuxakalal...\n",
            "Name: zu, Length: 4739, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuOloxbRTxSr",
        "outputId": "465a506d-5b6f-4bc0-ba90-342d96453ff0"
      },
      "source": [
        "# BPETokenizer training\n",
        "english_vocab_size_bpe = 10000\n",
        "zulu_vocab_size_bpe = 10000\n",
        "\n",
        "tokenizer_en = Toke(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer_zu = Toke(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "trainer_en = BpeTrainer(vocab_size = english_vocab_size_bpe, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "trainer_zu = BpeTrainer(vocab_size = zulu_vocab_size_bpe, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "tokenizer_zu.pre_tokenizer = Whitespace()\n",
        "\n",
        "#files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "tokenizer_en.train_from_iterator(english_sentences, trainer_en)\n",
        "tokenizer_zu.train_from_iterator(zulu_sentences, trainer_zu)\n",
        "\n",
        "output = tokenizer_en.encode((\"Hello, y'all! How are you üòÅ ?\").lower())\n",
        "output1 = tokenizer_en.encode((\"abukho ubuholi obungasusa abaphikisana nama-semites.\").lower())\n",
        "print(output.tokens)\n",
        "print(output1.tokens)\n",
        "outputz = tokenizer_zu.encode((\"Hello, y'all! How are you üòÅ ?\").lower())\n",
        "outputz1 = tokenizer_zu.encode((\"abukho ubuholi obungasusa abaphikisana nama-semites.\").lower())\n",
        "print(outputz.tokens)\n",
        "print(outputz1.tokens)\n",
        "print(tokenizer_en.token_to_id(\"hel\"))\n",
        "print(tokenizer_en.id_to_token(1))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?']\n",
            "['ab', 'uk', 'ho', 'u', 'bu', 'hol', 'i', 'ob', 'un', 'gas', 'usa', 'ab', 'ap', 'hik', 'is', 'ana', 'nam', 'a', '-', 'semit', 'es', '.']\n",
            "['hel', 'lo', ',', 'y', \"'\", 'all', '!', 'ho', 'w', 'are', 'you', '[UNK]', '?']\n",
            "['abukho', 'ubuholi', 'obunga', 'susa', 'abaphikisana', 'nama', '-', 'semi', 'tes', '.']\n",
            "511\n",
            "[CLS]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLg8NjIE5fNL",
        "outputId": "8e25bc8e-90c4-4680-8535-932501f21f80"
      },
      "source": [
        "# Remove empty rows and maybe rows that have sequences of length longer than x\n",
        "length_counts_en = defaultdict(lambda: 0)\n",
        "length_counts_zu = defaultdict(lambda: 0)\n",
        "inds_to_remove = []\n",
        "for i in range(len(english_sentences)):\n",
        "  length_counts_en[len(english_sentences[i].split())] += 1\n",
        "  length_counts_zu[len(zulu_sentences[i].split())] += 1\n",
        "  if len(english_sentences[i].split()) > 20 or len(zulu_sentences[i].split()) > 20:\n",
        "    inds_to_remove.append(i)\n",
        "\n",
        "old_eng_sent = english_sentences\n",
        "old_zu_sent = zulu_sentences\n",
        "english_sentences = english_sentences.drop(inds_to_remove, axis=0)\n",
        "zulu_sentences = zulu_sentences.drop(inds_to_remove, axis=0)\n",
        "\n",
        "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
        "zulu_words_counter = collections.Counter([word for sentence in zulu_sentences for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} Zulu words.'.format(len([word for sentence in zulu_sentences for word in sentence.split()])))\n",
        "print('{} unique Zulu words.'.format(len(zulu_words_counter)))\n",
        "print('10 Most common words in the Zulu dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*zulu_words_counter.most_common(10)))[0]) + '\"')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27271 English words.\n",
            "9538 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"the\" \"to\" \"a\" \"and\" \"in\" \"of\" \"is\" \"for\" \"was\" \"on\"\n",
            "\n",
            "21288 Zulu words.\n",
            "14241 unique Zulu words.\n",
            "10 Most common words in the Zulu dataset:\n",
            "\"ukuthi\" \"futhi\" \"kakhulu\" \"noma\" \"kodwa\" \"uma\" \"kusho\" \"kanye\" \"-\" \"kube\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2-OoeLB5RWx"
      },
      "source": [
        "def tokenize(x):\n",
        "    \"\"\"\n",
        "    Tokenize x\n",
        "    :param x: List of sentences/strings to be tokenized\n",
        "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer\n",
        "\n",
        "def bpe_tokenize(x, tokenizer_to_use):\n",
        "  # return the bpe tokenization of each sentence as a sequence\n",
        "  all_seq = []\n",
        "  for curr_sent in x:\n",
        "    otp = tokenizer_to_use.encode(curr_sent)\n",
        "    curr_tokenization = otp.tokens\n",
        "    curr_seq = []\n",
        "    for curr_token in curr_tokenization:\n",
        "      curr_seq.append(tokenizer_to_use.token_to_id(curr_token))\n",
        "\n",
        "    all_seq.append(curr_seq)\n",
        "  \n",
        "  return all_seq"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKbuF9xa8rhD"
      },
      "source": [
        "def pad(x, length=None):\n",
        "    \"\"\"\n",
        "    Pad x\n",
        "    :param x: List of sequences.\n",
        "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
        "    :return: Padded numpy array of sequences\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "    return pad_sequences(x, maxlen=length, padding='post')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDwhpfrC8wAZ",
        "outputId": "a635f111-7c73-4cc6-b2d7-7f2150775a7a"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    \"\"\"\n",
        "    Preprocess x and y\n",
        "    :param x: Feature List of sentences\n",
        "    :param y: Label List of sentences\n",
        "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
        "    \"\"\"\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x_bpe = bpe_tokenize(x, tokenizer_en)\n",
        "    preprocess_y_bpe = bpe_tokenize(y, tokenizer_zu)\n",
        "\n",
        "    #Print min and max values of zulu sentences\n",
        "    min_zu = 2000\n",
        "    max_zu = 0\n",
        "    for sample_i, (sent, token_sent) in enumerate(zip(y, preprocess_y_bpe)):\n",
        "      if len(token_sent) != 0:\n",
        "        min_zu = min(min_zu, min(token_sent))\n",
        "        max_zu = max(max_zu, max(token_sent))\n",
        "    print('Min:', min_zu, ' Max:', max_zu)\n",
        "\n",
        "\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    preprocess_x_bpe = pad(preprocess_x_bpe)\n",
        "    preprocess_y_bpe = pad(preprocess_y_bpe)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "    preprocess_y_bpe = preprocess_y_bpe.reshape(*preprocess_y_bpe.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk, preprocess_x_bpe, preprocess_y_bpe\n",
        "\n",
        "preproc_english_sentences, preproc_zulu_sentences, english_tokenizer, zulu_tokenizer, preproc_english_sentences_bpe, preproc_zulu_sentences_bpe =\\\n",
        "    preprocess(english_sentences, zulu_sentences)\n",
        "    \n",
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_zulu_sequence_length = preproc_zulu_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "zulu_vocab_size = len(zulu_tokenizer.word_index)\n",
        "\n",
        "max_english_sequence_length_bpe = preproc_english_sentences_bpe.shape[1]\n",
        "max_zulu_sequence_length_bpe = preproc_zulu_sentences_bpe.shape[1]\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max Zulu sentence length:\", max_zulu_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"Zulu vocabulary size:\", zulu_vocab_size)\n",
        "\n",
        "print()\n",
        "print(\"Max English sentence length(bpe):\", max_english_sequence_length_bpe)\n",
        "print(\"Max Zulu sentence length(bpe):\", max_zulu_sequence_length_bpe)\n",
        "print(\"English vocabulary size(bpe):\", english_vocab_size_bpe)\n",
        "print(\"Zulu vocabulary size(bpe):\", zulu_vocab_size_bpe)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 5  Max: 9998\n",
            "Data Preprocessed\n",
            "Max English sentence length: 23\n",
            "Max Zulu sentence length: 25\n",
            "English vocabulary size: 7639\n",
            "Zulu vocabulary size: 12597\n",
            "\n",
            "Max English sentence length(bpe): 42\n",
            "Max Zulu sentence length(bpe): 58\n",
            "English vocabulary size(bpe): 10000\n",
            "Zulu vocabulary size(bpe): 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyDsLcbe9Je2",
        "outputId": "7e02b10c-5892-4293-99d7-bddbcde327f7"
      },
      "source": [
        "def logits_to_text(logits, tokenizer):\n",
        "    \"\"\"\n",
        "    Turn logits from a neural network into text using the tokenizer\n",
        "    :param logits: Logits from a neural network\n",
        "    :param tokenizer: Keras Tokenizer fit on the labels\n",
        "    :return: String that represents the text of the logits\n",
        "    \"\"\"\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<PAD>'\n",
        "\n",
        "    #print(\"ITW:\", index_to_words)\n",
        "    #print(\"ITW[42]:\", index_to_words[42])\n",
        "    print(logits, np.shape(logits))\n",
        "    #print([index_to_words[prediction[0]] for prediction in logits])\n",
        "    print([prediction for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
        "    #return ' '.join([index_to_words[prediction[0]] for prediction in logits])\n",
        "\n",
        "\n",
        "print('`logits_to_text` function loaded.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`logits_to_text` function loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xGA6cA6kFiT"
      },
      "source": [
        "def logits_to_text_bpe(logits, tokenizer):\n",
        "  #return ' '.join([tokenizer.id_to_token(prediction) for prediction in logits])\n",
        "  return ' '.join([tokenizer.id_to_token(prediction) for prediction in np.argmax(logits, 1)])\n",
        "\n",
        "#   print(preproc_english_sentences_bpe[0])\n",
        "# print(logits_to_text_bpe(preproc_english_sentences_bpe[0], tokenizer_en))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lekqixg39OOo"
      },
      "source": [
        "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a basic RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size+1, activation='softmax'))) \n",
        "    #model.add(TimeDistributed(Dense(2500, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiWe-tJDuJC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e19a17-e5c6-4ab5-dcb9-c0ca5b333eb2"
      },
      "source": [
        "#tests.test_simple_model(simple_model)\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x = pad(preproc_english_sentences, max_zulu_sequence_length)\n",
        "tmp_x = tmp_x.reshape((-1, preproc_zulu_sentences.shape[-2], 1))\n",
        "\n",
        "#print('Tmp_x', tmp_x[0], np.shape(tmp_x))\n",
        "#print(logits_to_text(tmp_x[0], english_tokenizer))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model = simple_model(\n",
        "    tmp_x.shape,\n",
        "    max_zulu_sequence_length,\n",
        "    english_vocab_size,\n",
        "    zulu_vocab_size)\n",
        "\n",
        "print(simple_rnn_model.summary())\n",
        "\n",
        "simple_rnn_model.fit(tmp_x, preproc_zulu_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], zulu_tokenizer))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_9 (GRU)                 (None, 25, 256)           198912    \n",
            "                                                                 \n",
            " time_distributed_16 (TimeDi  (None, 25, 1024)         263168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 25, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_17 (TimeDi  (None, 25, 12598)        12912950  \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,375,030\n",
            "Trainable params: 13,375,030\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 3s 854ms/step - loss: 8.9886 - accuracy: 0.2610 - val_loss: 7.4507 - val_accuracy: 0.5756\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 1s 444ms/step - loss: 6.0494 - accuracy: 0.5943 - val_loss: 5.1591 - val_accuracy: 0.5756\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 1s 448ms/step - loss: 4.2420 - accuracy: 0.5943 - val_loss: 4.7411 - val_accuracy: 0.5756\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 1s 448ms/step - loss: 4.0014 - accuracy: 0.5943 - val_loss: 4.7512 - val_accuracy: 0.5756\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 1s 445ms/step - loss: 3.9924 - accuracy: 0.5943 - val_loss: 4.6823 - val_accuracy: 0.5756\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 3.8813 - accuracy: 0.5943 - val_loss: 4.7274 - val_accuracy: 0.5756\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 1s 446ms/step - loss: 3.7989 - accuracy: 0.5943 - val_loss: 4.9290 - val_accuracy: 0.5756\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 3.7844 - accuracy: 0.5947 - val_loss: 4.9560 - val_accuracy: 0.5772\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 1s 447ms/step - loss: 3.7530 - accuracy: 0.5955 - val_loss: 5.0252 - val_accuracy: 0.5756\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 1s 445ms/step - loss: 3.7250 - accuracy: 0.5955 - val_loss: 5.0028 - val_accuracy: 0.5762\n",
            "[[2.7942453e-03 1.2077056e-03 1.3841715e-03 ... 5.0579706e-06\n",
            "  5.0243125e-06 5.0240969e-06]\n",
            " [8.4640998e-03 4.1886927e-03 4.7959164e-03 ... 1.0381899e-06\n",
            "  1.0303651e-06 1.1037392e-06]\n",
            " [8.4732976e-03 4.1935123e-03 4.8008380e-03 ... 1.0376998e-06\n",
            "  1.0297069e-06 1.1031688e-06]\n",
            " ...\n",
            " [9.9688369e-01 1.5367143e-04 3.5791902e-04 ... 1.3980521e-12\n",
            "  1.2006900e-12 9.8791125e-13]\n",
            " [9.9848217e-01 8.1142112e-05 1.9552722e-04 ... 3.2331795e-13\n",
            "  2.7189668e-13 2.2219345e-13]\n",
            " [9.9919862e-01 4.5622208e-05 1.1358611e-04 ... 8.5988657e-14\n",
            "  7.1008364e-14 5.7789216e-14]] (25, 12598)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ2McZASuQQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89045dd7-fcc8-44a8-df13-6aef26d3192b"
      },
      "source": [
        "#tests.test_simple_model(simple_model)\n",
        "\n",
        "# Reshaping the input to work with a basic RNN\n",
        "tmp_x_bpe = pad(preproc_english_sentences_bpe, max_zulu_sequence_length_bpe)\n",
        "tmp_x_bpe = tmp_x_bpe.reshape((-1, preproc_zulu_sentences_bpe.shape[-2], 1))\n",
        "\n",
        "#print('Tmp_x', tmp_x[0], np.shape(tmp_x))\n",
        "#print(logits_to_text(tmp_x[0], english_tokenizer))\n",
        "\n",
        "# Train the neural network\n",
        "simple_rnn_model_bpe = simple_model(\n",
        "    tmp_x_bpe.shape,\n",
        "    max_zulu_sequence_length_bpe,\n",
        "    english_vocab_size_bpe,\n",
        "    zulu_vocab_size_bpe)\n",
        "\n",
        "print(simple_rnn_model_bpe.summary())\n",
        "\n",
        "simple_rnn_model_bpe.fit(tmp_x_bpe, preproc_zulu_sentences_bpe, batch_size=1024, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Print prediction(s)\n",
        "print(logits_to_text_bpe(simple_rnn_model_bpe.predict(tmp_x_bpe[:1])[0], tokenizer_zu))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_10 (GRU)                (None, 58, 256)           198912    \n",
            "                                                                 \n",
            " time_distributed_18 (TimeDi  (None, 58, 1024)         263168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 58, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_19 (TimeDi  (None, 58, 10001)        10251025  \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,713,105\n",
            "Trainable params: 10,713,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 1s/step - loss: 9.0372 - accuracy: 0.3764 - val_loss: 8.3651 - val_accuracy: 0.6699\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 2s 777ms/step - loss: 6.4744 - accuracy: 0.6758 - val_loss: 3.3355 - val_accuracy: 0.6699\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 2s 780ms/step - loss: 2.8877 - accuracy: 0.6810 - val_loss: 3.3506 - val_accuracy: 0.6699\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 2s 778ms/step - loss: 2.8615 - accuracy: 0.6797 - val_loss: 3.2149 - val_accuracy: 0.6638\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 2s 778ms/step - loss: 2.8257 - accuracy: 0.6705 - val_loss: 3.1088 - val_accuracy: 0.6611\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 2s 779ms/step - loss: 2.7155 - accuracy: 0.6693 - val_loss: 3.1816 - val_accuracy: 0.6671\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 2s 780ms/step - loss: 2.6693 - accuracy: 0.6783 - val_loss: 3.0543 - val_accuracy: 0.6713\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 2s 779ms/step - loss: 2.5990 - accuracy: 0.6794 - val_loss: 2.9510 - val_accuracy: 0.6715\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 2s 779ms/step - loss: 2.6025 - accuracy: 0.6807 - val_loss: 3.0412 - val_accuracy: 0.6699\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 2s 779ms/step - loss: 2.5895 - accuracy: 0.6808 - val_loss: 3.0465 - val_accuracy: 0.6699\n",
            "[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdNIDpNrL6Oz"
      },
      "source": [
        "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a RNN model using word embedding on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(GRU(256, return_sequences=True))    \n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#tests.test_embed_model(embed_model)\n",
        "\n",
        "# TODO: Reshape the input\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC3D81I3nRJp",
        "outputId": "4642400e-0bd6-49bd-edff-5f9ce78129b6"
      },
      "source": [
        "tmp_x = pad(preproc_english_sentences, preproc_zulu_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_zulu_sentences.shape[-2]))\n",
        "\n",
        "# TODO: Train the neural network\n",
        "embed_rnn_model = embed_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_zulu_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(zulu_tokenizer.word_index)+1)\n",
        "\n",
        "embed_rnn_model.summary()\n",
        "\n",
        "embed_rnn_model.fit(tmp_x, preproc_zulu_sentences, batch_size=1024, epochs=200, validation_split=0.2)\n",
        "\n",
        "# TODO: Print prediction(s)\n",
        "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], zulu_tokenizer))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 25, 256)           1955840   \n",
            "                                                                 \n",
            " gru_11 (GRU)                (None, 25, 256)           394752    \n",
            "                                                                 \n",
            " time_distributed_20 (TimeDi  (None, 25, 1024)         263168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 25, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_21 (TimeDi  (None, 25, 12598)        12912950  \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,526,710\n",
            "Trainable params: 15,526,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 3s 820ms/step - loss: 9.3793 - accuracy: 0.2615 - val_loss: 8.2462 - val_accuracy: 0.5756\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 6.6159 - accuracy: 0.5943 - val_loss: 5.4124 - val_accuracy: 0.5756\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 5.3260 - accuracy: 0.5943 - val_loss: 6.0482 - val_accuracy: 0.5756\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 1s 459ms/step - loss: 5.6872 - accuracy: 0.5943 - val_loss: 6.0699 - val_accuracy: 0.5757\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 5.4207 - accuracy: 0.5938 - val_loss: 7.0515 - val_accuracy: 0.0060\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 5.5865 - accuracy: 0.2631 - val_loss: 5.3137 - val_accuracy: 0.5713\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 4.3800 - accuracy: 0.5889 - val_loss: 4.9911 - val_accuracy: 0.5756\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 4.3292 - accuracy: 0.5943 - val_loss: 4.9444 - val_accuracy: 0.5756\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 4.1449 - accuracy: 0.5943 - val_loss: 4.9673 - val_accuracy: 0.5756\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 4.0467 - accuracy: 0.5943 - val_loss: 5.0040 - val_accuracy: 0.5754\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.9115 - accuracy: 0.5943 - val_loss: 4.9897 - val_accuracy: 0.5752\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 3.8556 - accuracy: 0.5942 - val_loss: 5.0351 - val_accuracy: 0.5753\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.7960 - accuracy: 0.5941 - val_loss: 5.0792 - val_accuracy: 0.5755\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.7524 - accuracy: 0.5942 - val_loss: 5.0890 - val_accuracy: 0.5756\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 3.7190 - accuracy: 0.5942 - val_loss: 5.1085 - val_accuracy: 0.5752\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 3.6667 - accuracy: 0.5944 - val_loss: 5.1304 - val_accuracy: 0.5757\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 3.6266 - accuracy: 0.5947 - val_loss: 5.1513 - val_accuracy: 0.5757\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.5863 - accuracy: 0.5947 - val_loss: 5.1893 - val_accuracy: 0.5755\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.5513 - accuracy: 0.5947 - val_loss: 5.2042 - val_accuracy: 0.5753\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 3.5123 - accuracy: 0.5953 - val_loss: 5.2433 - val_accuracy: 0.5752\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.4698 - accuracy: 0.5960 - val_loss: 5.2773 - val_accuracy: 0.5753\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.4255 - accuracy: 0.5961 - val_loss: 5.3023 - val_accuracy: 0.5756\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.3919 - accuracy: 0.5963 - val_loss: 5.3592 - val_accuracy: 0.5760\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.3649 - accuracy: 0.5964 - val_loss: 5.3710 - val_accuracy: 0.5762\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 3.3163 - accuracy: 0.5967 - val_loss: 5.4011 - val_accuracy: 0.5763\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.2742 - accuracy: 0.5966 - val_loss: 5.4323 - val_accuracy: 0.5756\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.2317 - accuracy: 0.5966 - val_loss: 5.4593 - val_accuracy: 0.5758\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.1907 - accuracy: 0.5968 - val_loss: 5.4901 - val_accuracy: 0.5755\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.1503 - accuracy: 0.5970 - val_loss: 5.5224 - val_accuracy: 0.5756\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 3.1022 - accuracy: 0.5970 - val_loss: 5.5501 - val_accuracy: 0.5758\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.0605 - accuracy: 0.5972 - val_loss: 5.5858 - val_accuracy: 0.5748\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 3.0128 - accuracy: 0.5971 - val_loss: 5.6119 - val_accuracy: 0.5750\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 2.9703 - accuracy: 0.5973 - val_loss: 5.6410 - val_accuracy: 0.5737\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 2.9216 - accuracy: 0.5982 - val_loss: 5.6627 - val_accuracy: 0.5743\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 2.8737 - accuracy: 0.5979 - val_loss: 5.6864 - val_accuracy: 0.5736\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 2.8219 - accuracy: 0.5990 - val_loss: 5.7133 - val_accuracy: 0.5709\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 2.7793 - accuracy: 0.5988 - val_loss: 5.7330 - val_accuracy: 0.5730\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 2.7341 - accuracy: 0.5995 - val_loss: 5.7568 - val_accuracy: 0.5706\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 2.6802 - accuracy: 0.6005 - val_loss: 5.7803 - val_accuracy: 0.5691\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 1s 459ms/step - loss: 2.6239 - accuracy: 0.6016 - val_loss: 5.7995 - val_accuracy: 0.5710\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 2.5769 - accuracy: 0.6023 - val_loss: 5.8206 - val_accuracy: 0.5691\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 2.5236 - accuracy: 0.6040 - val_loss: 5.8494 - val_accuracy: 0.5646\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 2.4791 - accuracy: 0.6058 - val_loss: 5.8565 - val_accuracy: 0.5684\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 2.4212 - accuracy: 0.6088 - val_loss: 5.8672 - val_accuracy: 0.5684\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 2.3641 - accuracy: 0.6119 - val_loss: 5.9020 - val_accuracy: 0.5610\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 2.3170 - accuracy: 0.6141 - val_loss: 5.9063 - val_accuracy: 0.5676\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 2.2704 - accuracy: 0.6164 - val_loss: 5.9263 - val_accuracy: 0.5625\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 2.2196 - accuracy: 0.6209 - val_loss: 5.9300 - val_accuracy: 0.5639\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 2.1543 - accuracy: 0.6274 - val_loss: 5.9455 - val_accuracy: 0.5654\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 2.1128 - accuracy: 0.6304 - val_loss: 5.9662 - val_accuracy: 0.5588\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 2.0671 - accuracy: 0.6354 - val_loss: 5.9733 - val_accuracy: 0.5647\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 2.0058 - accuracy: 0.6407 - val_loss: 5.9920 - val_accuracy: 0.5579\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 1.9543 - accuracy: 0.6475 - val_loss: 6.0031 - val_accuracy: 0.5606\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 1.9052 - accuracy: 0.6547 - val_loss: 6.0183 - val_accuracy: 0.5611\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 1.8497 - accuracy: 0.6611 - val_loss: 6.0281 - val_accuracy: 0.5574\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 1.8025 - accuracy: 0.6668 - val_loss: 6.0432 - val_accuracy: 0.5603\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 1.7470 - accuracy: 0.6753 - val_loss: 6.0493 - val_accuracy: 0.5576\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 1.6942 - accuracy: 0.6831 - val_loss: 6.0735 - val_accuracy: 0.5560\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 1.6510 - accuracy: 0.6873 - val_loss: 6.0679 - val_accuracy: 0.5600\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 1.6056 - accuracy: 0.6946 - val_loss: 6.0985 - val_accuracy: 0.5582\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 1.5631 - accuracy: 0.7028 - val_loss: 6.1032 - val_accuracy: 0.5516\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 1s 460ms/step - loss: 1.5249 - accuracy: 0.7090 - val_loss: 6.1119 - val_accuracy: 0.5593\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 1.4780 - accuracy: 0.7151 - val_loss: 6.1121 - val_accuracy: 0.5574\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 1.4344 - accuracy: 0.7230 - val_loss: 6.1328 - val_accuracy: 0.5543\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 1.3940 - accuracy: 0.7300 - val_loss: 6.1279 - val_accuracy: 0.5599\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 1.3517 - accuracy: 0.7366 - val_loss: 6.1470 - val_accuracy: 0.5552\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 1.3043 - accuracy: 0.7453 - val_loss: 6.1670 - val_accuracy: 0.5515\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 1.2678 - accuracy: 0.7532 - val_loss: 6.1524 - val_accuracy: 0.5597\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 1.2315 - accuracy: 0.7572 - val_loss: 6.1874 - val_accuracy: 0.5521\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 1.1922 - accuracy: 0.7658 - val_loss: 6.1790 - val_accuracy: 0.5551\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 1.1555 - accuracy: 0.7729 - val_loss: 6.1933 - val_accuracy: 0.5560\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 1.1205 - accuracy: 0.7804 - val_loss: 6.2172 - val_accuracy: 0.5511\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 1.0909 - accuracy: 0.7851 - val_loss: 6.2096 - val_accuracy: 0.5561\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 1.0546 - accuracy: 0.7905 - val_loss: 6.2291 - val_accuracy: 0.5516\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 1.0293 - accuracy: 0.7970 - val_loss: 6.2196 - val_accuracy: 0.5552\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 1.0024 - accuracy: 0.8006 - val_loss: 6.2403 - val_accuracy: 0.5542\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.9725 - accuracy: 0.8053 - val_loss: 6.2404 - val_accuracy: 0.5545\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.9427 - accuracy: 0.8125 - val_loss: 6.2459 - val_accuracy: 0.5558\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.9139 - accuracy: 0.8183 - val_loss: 6.2633 - val_accuracy: 0.5532\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.8860 - accuracy: 0.8249 - val_loss: 6.2635 - val_accuracy: 0.5547\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.8653 - accuracy: 0.8278 - val_loss: 6.2830 - val_accuracy: 0.5527\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.8463 - accuracy: 0.8313 - val_loss: 6.2864 - val_accuracy: 0.5525\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.8204 - accuracy: 0.8371 - val_loss: 6.2850 - val_accuracy: 0.5559\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.8040 - accuracy: 0.8386 - val_loss: 6.3078 - val_accuracy: 0.5521\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.7913 - accuracy: 0.8418 - val_loss: 6.2985 - val_accuracy: 0.5554\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.7721 - accuracy: 0.8469 - val_loss: 6.3118 - val_accuracy: 0.5542\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 0.7483 - accuracy: 0.8513 - val_loss: 6.3216 - val_accuracy: 0.5518\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.7368 - accuracy: 0.8531 - val_loss: 6.3073 - val_accuracy: 0.5567\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.7175 - accuracy: 0.8575 - val_loss: 6.3297 - val_accuracy: 0.5528\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.7037 - accuracy: 0.8623 - val_loss: 6.3245 - val_accuracy: 0.5560\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.6899 - accuracy: 0.8634 - val_loss: 6.3290 - val_accuracy: 0.5552\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.6697 - accuracy: 0.8679 - val_loss: 6.3428 - val_accuracy: 0.5543\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.6588 - accuracy: 0.8703 - val_loss: 6.3415 - val_accuracy: 0.5563\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.6478 - accuracy: 0.8720 - val_loss: 6.3656 - val_accuracy: 0.5520\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.6295 - accuracy: 0.8767 - val_loss: 6.3562 - val_accuracy: 0.5567\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.6214 - accuracy: 0.8774 - val_loss: 6.3718 - val_accuracy: 0.5538\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.6068 - accuracy: 0.8812 - val_loss: 6.3727 - val_accuracy: 0.5556\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.5930 - accuracy: 0.8835 - val_loss: 6.3807 - val_accuracy: 0.5540\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.5828 - accuracy: 0.8862 - val_loss: 6.3874 - val_accuracy: 0.5531\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.5710 - accuracy: 0.8892 - val_loss: 6.3846 - val_accuracy: 0.5563\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.5634 - accuracy: 0.8907 - val_loss: 6.4036 - val_accuracy: 0.5522\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.5537 - accuracy: 0.8926 - val_loss: 6.3903 - val_accuracy: 0.5572\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.5454 - accuracy: 0.8943 - val_loss: 6.4170 - val_accuracy: 0.5518\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.5318 - accuracy: 0.8983 - val_loss: 6.4009 - val_accuracy: 0.5555\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.5288 - accuracy: 0.8969 - val_loss: 6.4173 - val_accuracy: 0.5537\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.5184 - accuracy: 0.9002 - val_loss: 6.4234 - val_accuracy: 0.5533\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.5087 - accuracy: 0.9018 - val_loss: 6.4169 - val_accuracy: 0.5550\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.4974 - accuracy: 0.9033 - val_loss: 6.4390 - val_accuracy: 0.5524\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.4929 - accuracy: 0.9055 - val_loss: 6.4280 - val_accuracy: 0.5556\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.4868 - accuracy: 0.9062 - val_loss: 6.4503 - val_accuracy: 0.5515\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.4781 - accuracy: 0.9081 - val_loss: 6.4288 - val_accuracy: 0.5567\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.4746 - accuracy: 0.9101 - val_loss: 6.4510 - val_accuracy: 0.5521\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.4653 - accuracy: 0.9118 - val_loss: 6.4298 - val_accuracy: 0.5562\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.4608 - accuracy: 0.9127 - val_loss: 6.4633 - val_accuracy: 0.5509\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.4514 - accuracy: 0.9155 - val_loss: 6.4381 - val_accuracy: 0.5573\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.4503 - accuracy: 0.9157 - val_loss: 6.4622 - val_accuracy: 0.5520\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.4427 - accuracy: 0.9174 - val_loss: 6.4431 - val_accuracy: 0.5572\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.4366 - accuracy: 0.9185 - val_loss: 6.4743 - val_accuracy: 0.5500\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.4382 - accuracy: 0.9167 - val_loss: 6.4382 - val_accuracy: 0.5571\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.4298 - accuracy: 0.9191 - val_loss: 6.4725 - val_accuracy: 0.5514\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.4240 - accuracy: 0.9201 - val_loss: 6.4472 - val_accuracy: 0.5575\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.4187 - accuracy: 0.9217 - val_loss: 6.4746 - val_accuracy: 0.5519\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.4108 - accuracy: 0.9253 - val_loss: 6.4500 - val_accuracy: 0.5578\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.4105 - accuracy: 0.9240 - val_loss: 6.4899 - val_accuracy: 0.5514\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.4073 - accuracy: 0.9247 - val_loss: 6.4562 - val_accuracy: 0.5565\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.3990 - accuracy: 0.9265 - val_loss: 6.4792 - val_accuracy: 0.5527\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.3969 - accuracy: 0.9277 - val_loss: 6.4763 - val_accuracy: 0.5545\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 1s 451ms/step - loss: 0.3935 - accuracy: 0.9269 - val_loss: 6.4738 - val_accuracy: 0.5547\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3868 - accuracy: 0.9298 - val_loss: 6.4898 - val_accuracy: 0.5520\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3815 - accuracy: 0.9295 - val_loss: 6.4739 - val_accuracy: 0.5564\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.3785 - accuracy: 0.9314 - val_loss: 6.5011 - val_accuracy: 0.5523\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3756 - accuracy: 0.9332 - val_loss: 6.4837 - val_accuracy: 0.5557\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3671 - accuracy: 0.9350 - val_loss: 6.4909 - val_accuracy: 0.5544\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3678 - accuracy: 0.9342 - val_loss: 6.5019 - val_accuracy: 0.5531\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.3646 - accuracy: 0.9346 - val_loss: 6.4928 - val_accuracy: 0.5552\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3632 - accuracy: 0.9352 - val_loss: 6.5142 - val_accuracy: 0.5519\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3575 - accuracy: 0.9375 - val_loss: 6.4974 - val_accuracy: 0.5551\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3556 - accuracy: 0.9375 - val_loss: 6.5070 - val_accuracy: 0.5533\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 1s 459ms/step - loss: 0.3554 - accuracy: 0.9370 - val_loss: 6.5006 - val_accuracy: 0.5541\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.3536 - accuracy: 0.9388 - val_loss: 6.4970 - val_accuracy: 0.5549\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.3468 - accuracy: 0.9399 - val_loss: 6.5092 - val_accuracy: 0.5540\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.3446 - accuracy: 0.9416 - val_loss: 6.4949 - val_accuracy: 0.5567\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.3445 - accuracy: 0.9409 - val_loss: 6.5151 - val_accuracy: 0.5545\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.3388 - accuracy: 0.9416 - val_loss: 6.5120 - val_accuracy: 0.5553\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.3365 - accuracy: 0.9429 - val_loss: 6.5109 - val_accuracy: 0.5550\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3351 - accuracy: 0.9424 - val_loss: 6.5219 - val_accuracy: 0.5540\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3332 - accuracy: 0.9427 - val_loss: 6.5189 - val_accuracy: 0.5548\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.3314 - accuracy: 0.9434 - val_loss: 6.5311 - val_accuracy: 0.5532\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.3267 - accuracy: 0.9449 - val_loss: 6.5270 - val_accuracy: 0.5534\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3270 - accuracy: 0.9448 - val_loss: 6.5243 - val_accuracy: 0.5548\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 1s 461ms/step - loss: 0.3235 - accuracy: 0.9455 - val_loss: 6.5388 - val_accuracy: 0.5530\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3213 - accuracy: 0.9457 - val_loss: 6.5351 - val_accuracy: 0.5534\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3183 - accuracy: 0.9462 - val_loss: 6.5386 - val_accuracy: 0.5538\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 1s 459ms/step - loss: 0.3162 - accuracy: 0.9468 - val_loss: 6.5480 - val_accuracy: 0.5533\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3175 - accuracy: 0.9471 - val_loss: 6.5415 - val_accuracy: 0.5545\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3131 - accuracy: 0.9476 - val_loss: 6.5550 - val_accuracy: 0.5535\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3119 - accuracy: 0.9492 - val_loss: 6.5558 - val_accuracy: 0.5539\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3105 - accuracy: 0.9494 - val_loss: 6.5599 - val_accuracy: 0.5539\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.3123 - accuracy: 0.9481 - val_loss: 6.5651 - val_accuracy: 0.5536\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3112 - accuracy: 0.9488 - val_loss: 6.5585 - val_accuracy: 0.5543\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3090 - accuracy: 0.9489 - val_loss: 6.5632 - val_accuracy: 0.5540\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3027 - accuracy: 0.9509 - val_loss: 6.5626 - val_accuracy: 0.5549\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.3034 - accuracy: 0.9506 - val_loss: 6.5702 - val_accuracy: 0.5541\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.3013 - accuracy: 0.9505 - val_loss: 6.5685 - val_accuracy: 0.5538\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 1s 459ms/step - loss: 0.2989 - accuracy: 0.9511 - val_loss: 6.5578 - val_accuracy: 0.5553\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2987 - accuracy: 0.9520 - val_loss: 6.5699 - val_accuracy: 0.5544\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.2953 - accuracy: 0.9522 - val_loss: 6.5739 - val_accuracy: 0.5545\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.2982 - accuracy: 0.9515 - val_loss: 6.5714 - val_accuracy: 0.5550\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2944 - accuracy: 0.9529 - val_loss: 6.5749 - val_accuracy: 0.5545\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.2932 - accuracy: 0.9520 - val_loss: 6.5743 - val_accuracy: 0.5549\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2905 - accuracy: 0.9540 - val_loss: 6.5828 - val_accuracy: 0.5539\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2917 - accuracy: 0.9528 - val_loss: 6.5789 - val_accuracy: 0.5550\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 1s 458ms/step - loss: 0.2896 - accuracy: 0.9543 - val_loss: 6.5828 - val_accuracy: 0.5547\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2881 - accuracy: 0.9542 - val_loss: 6.5815 - val_accuracy: 0.5548\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2890 - accuracy: 0.9540 - val_loss: 6.5883 - val_accuracy: 0.5538\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2845 - accuracy: 0.9547 - val_loss: 6.5866 - val_accuracy: 0.5540\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.2843 - accuracy: 0.9551 - val_loss: 6.5858 - val_accuracy: 0.5545\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2834 - accuracy: 0.9549 - val_loss: 6.5881 - val_accuracy: 0.5549\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.2829 - accuracy: 0.9547 - val_loss: 6.5894 - val_accuracy: 0.5545\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2798 - accuracy: 0.9558 - val_loss: 6.5996 - val_accuracy: 0.5536\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2818 - accuracy: 0.9557 - val_loss: 6.5940 - val_accuracy: 0.5541\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2805 - accuracy: 0.9555 - val_loss: 6.5948 - val_accuracy: 0.5540\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 1s 452ms/step - loss: 0.2790 - accuracy: 0.9563 - val_loss: 6.6011 - val_accuracy: 0.5538\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2786 - accuracy: 0.9561 - val_loss: 6.5980 - val_accuracy: 0.5544\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2752 - accuracy: 0.9569 - val_loss: 6.6035 - val_accuracy: 0.5540\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 1s 461ms/step - loss: 0.2756 - accuracy: 0.9568 - val_loss: 6.6043 - val_accuracy: 0.5544\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 1s 455ms/step - loss: 0.2750 - accuracy: 0.9574 - val_loss: 6.6027 - val_accuracy: 0.5549\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2730 - accuracy: 0.9576 - val_loss: 6.6086 - val_accuracy: 0.5544\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2734 - accuracy: 0.9573 - val_loss: 6.6079 - val_accuracy: 0.5550\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2727 - accuracy: 0.9575 - val_loss: 6.6067 - val_accuracy: 0.5555\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2732 - accuracy: 0.9572 - val_loss: 6.6064 - val_accuracy: 0.5556\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.2719 - accuracy: 0.9579 - val_loss: 6.6045 - val_accuracy: 0.5552\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 1s 453ms/step - loss: 0.2699 - accuracy: 0.9587 - val_loss: 6.6124 - val_accuracy: 0.5539\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2694 - accuracy: 0.9579 - val_loss: 6.6165 - val_accuracy: 0.5537\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2687 - accuracy: 0.9589 - val_loss: 6.6201 - val_accuracy: 0.5539\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 1s 457ms/step - loss: 0.2697 - accuracy: 0.9581 - val_loss: 6.6210 - val_accuracy: 0.5540\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2658 - accuracy: 0.9595 - val_loss: 6.6209 - val_accuracy: 0.5539\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 1s 460ms/step - loss: 0.2650 - accuracy: 0.9596 - val_loss: 6.6233 - val_accuracy: 0.5544\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 1s 454ms/step - loss: 0.2641 - accuracy: 0.9591 - val_loss: 6.6278 - val_accuracy: 0.5538\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 1s 456ms/step - loss: 0.2654 - accuracy: 0.9589 - val_loss: 6.6173 - val_accuracy: 0.5546\n",
            "[[6.5643701e-04 1.1317666e-02 2.5120631e-04 ... 5.0070295e-11\n",
            "  3.5002602e-11 3.7798646e-11]\n",
            " [3.6514331e-07 1.4901340e-07 3.6989370e-15 ... 8.0145381e-24\n",
            "  7.4022045e-24 1.6748715e-23]\n",
            " [2.9640140e-07 1.6149279e-07 3.3147376e-06 ... 1.5998196e-22\n",
            "  2.1238925e-22 1.4243809e-22]\n",
            " ...\n",
            " [1.0000000e+00 2.4433253e-38 0.0000000e+00 ... 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00]\n",
            " [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00]\n",
            " [1.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00]] (25, 12598)\n",
            "[821, 819, 391, 6, 2544, 1264, 820, 1265, 2545, 2546, 1266, 5, 2547, 1267, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "uyise albhamu akhe e solo eminyaka yawo 60 angamanye anezingoma ezimnandi kakhulu kwengake ngazizwa <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW3apkl6mYlH",
        "outputId": "041349a5-bfc3-470a-acd8-8d8b2d3ffb37"
      },
      "source": [
        "tmp_x_bpe = pad(preproc_english_sentences_bpe, preproc_zulu_sentences_bpe.shape[1])\n",
        "tmp_x_bpe = tmp_x_bpe.reshape((-1, preproc_zulu_sentences_bpe.shape[-2]))\n",
        "\n",
        "# TODO: Train the neural network\n",
        "embed_rnn_model_bpe = embed_model(\n",
        "    tmp_x_bpe.shape,\n",
        "    preproc_zulu_sentences_bpe.shape[1],\n",
        "    english_vocab_size_bpe+1,\n",
        "    zulu_vocab_size_bpe+1)\n",
        "\n",
        "embed_rnn_model_bpe.summary()\n",
        "\n",
        "embed_rnn_model_bpe.fit(tmp_x_bpe, preproc_zulu_sentences_bpe, batch_size=1024, epochs=200, validation_split=0.2)\n",
        "\n",
        "# TODO: Print prediction(s)\n",
        "print(logits_to_text_bpe(embed_rnn_model_bpe.predict(tmp_x_bpe[:1])[0], tokenizer_zu))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 64, 256)           1280256   \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 64, 256)           394752    \n",
            "                                                                 \n",
            " time_distributed_8 (TimeDis  (None, 64, 1024)         263168    \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_9 (TimeDis  (None, 64, 5001)         5126025   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,064,201\n",
            "Trainable params: 7,064,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 3s 571ms/step - loss: 8.4854 - accuracy: 0.1328 - val_loss: 6.6103 - val_accuracy: 0.7143\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 5.8094 - accuracy: 0.7336 - val_loss: 3.5158 - val_accuracy: 0.7143\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 3.1593 - accuracy: 0.7336 - val_loss: 3.0638 - val_accuracy: 0.7144\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 1s 196ms/step - loss: 2.6046 - accuracy: 0.7286 - val_loss: 2.7193 - val_accuracy: 0.7154\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.3743 - accuracy: 0.7360 - val_loss: 2.6904 - val_accuracy: 0.7149\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.2902 - accuracy: 0.7358 - val_loss: 2.6113 - val_accuracy: 0.7147\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.2070 - accuracy: 0.7364 - val_loss: 2.5540 - val_accuracy: 0.7151\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.1715 - accuracy: 0.7372 - val_loss: 2.5549 - val_accuracy: 0.7149\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.1082 - accuracy: 0.7373 - val_loss: 2.5601 - val_accuracy: 0.7153\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 2.0747 - accuracy: 0.7367 - val_loss: 2.5026 - val_accuracy: 0.7152\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.0414 - accuracy: 0.7362 - val_loss: 2.5025 - val_accuracy: 0.7145\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 2.0010 - accuracy: 0.7357 - val_loss: 2.4738 - val_accuracy: 0.7151\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.9775 - accuracy: 0.7364 - val_loss: 2.5010 - val_accuracy: 0.7159\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.9585 - accuracy: 0.7384 - val_loss: 2.4595 - val_accuracy: 0.7160\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.9338 - accuracy: 0.7388 - val_loss: 2.4631 - val_accuracy: 0.7160\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 1s 196ms/step - loss: 1.9193 - accuracy: 0.7388 - val_loss: 2.4621 - val_accuracy: 0.7162\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.9031 - accuracy: 0.7394 - val_loss: 2.4836 - val_accuracy: 0.7165\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 1s 196ms/step - loss: 1.8845 - accuracy: 0.7395 - val_loss: 2.4975 - val_accuracy: 0.7161\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.8710 - accuracy: 0.7396 - val_loss: 2.5003 - val_accuracy: 0.7153\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.8521 - accuracy: 0.7398 - val_loss: 2.5134 - val_accuracy: 0.7153\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.8377 - accuracy: 0.7398 - val_loss: 2.5149 - val_accuracy: 0.7146\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.8245 - accuracy: 0.7408 - val_loss: 2.5513 - val_accuracy: 0.7130\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.8100 - accuracy: 0.7418 - val_loss: 2.5916 - val_accuracy: 0.7120\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 1s 196ms/step - loss: 1.7953 - accuracy: 0.7423 - val_loss: 2.6027 - val_accuracy: 0.7125\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.7819 - accuracy: 0.7425 - val_loss: 2.6013 - val_accuracy: 0.7103\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 1.7718 - accuracy: 0.7434 - val_loss: 2.6274 - val_accuracy: 0.7096\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.7593 - accuracy: 0.7441 - val_loss: 2.6613 - val_accuracy: 0.7098\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.7471 - accuracy: 0.7441 - val_loss: 2.6927 - val_accuracy: 0.7115\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.7384 - accuracy: 0.7446 - val_loss: 2.6946 - val_accuracy: 0.7102\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.7239 - accuracy: 0.7451 - val_loss: 2.7039 - val_accuracy: 0.7038\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 1.7335 - accuracy: 0.7449 - val_loss: 2.7304 - val_accuracy: 0.7092\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.7126 - accuracy: 0.7452 - val_loss: 2.7288 - val_accuracy: 0.7067\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.7005 - accuracy: 0.7463 - val_loss: 2.7428 - val_accuracy: 0.7066\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.6922 - accuracy: 0.7463 - val_loss: 2.7472 - val_accuracy: 0.7069\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.6784 - accuracy: 0.7471 - val_loss: 2.7721 - val_accuracy: 0.7093\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 1s 196ms/step - loss: 1.6721 - accuracy: 0.7466 - val_loss: 2.7577 - val_accuracy: 0.7029\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.6627 - accuracy: 0.7469 - val_loss: 2.8024 - val_accuracy: 0.7115\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.6464 - accuracy: 0.7466 - val_loss: 2.7936 - val_accuracy: 0.7069\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.6267 - accuracy: 0.7484 - val_loss: 2.8134 - val_accuracy: 0.7085\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.6115 - accuracy: 0.7485 - val_loss: 2.8317 - val_accuracy: 0.7082\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.5965 - accuracy: 0.7488 - val_loss: 2.8522 - val_accuracy: 0.7064\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.5815 - accuracy: 0.7494 - val_loss: 2.8925 - val_accuracy: 0.7093\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.5665 - accuracy: 0.7494 - val_loss: 2.8960 - val_accuracy: 0.7047\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.5532 - accuracy: 0.7508 - val_loss: 2.9340 - val_accuracy: 0.7083\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.5369 - accuracy: 0.7504 - val_loss: 2.9549 - val_accuracy: 0.7045\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 1.5226 - accuracy: 0.7514 - val_loss: 2.9812 - val_accuracy: 0.7072\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.5031 - accuracy: 0.7512 - val_loss: 2.9948 - val_accuracy: 0.7029\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 1.4893 - accuracy: 0.7522 - val_loss: 3.0318 - val_accuracy: 0.7084\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.4736 - accuracy: 0.7523 - val_loss: 3.0382 - val_accuracy: 0.7047\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 1.4523 - accuracy: 0.7547 - val_loss: 3.0668 - val_accuracy: 0.7078\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.4373 - accuracy: 0.7541 - val_loss: 3.0764 - val_accuracy: 0.7060\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.4232 - accuracy: 0.7551 - val_loss: 3.0843 - val_accuracy: 0.7045\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.4075 - accuracy: 0.7560 - val_loss: 3.1176 - val_accuracy: 0.7079\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 1.3944 - accuracy: 0.7564 - val_loss: 3.1062 - val_accuracy: 0.7023\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.3774 - accuracy: 0.7576 - val_loss: 3.1368 - val_accuracy: 0.7051\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 1.3530 - accuracy: 0.7588 - val_loss: 3.1403 - val_accuracy: 0.7046\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 1.3276 - accuracy: 0.7612 - val_loss: 3.1494 - val_accuracy: 0.7030\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.3152 - accuracy: 0.7621 - val_loss: 3.1822 - val_accuracy: 0.7059\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.2957 - accuracy: 0.7634 - val_loss: 3.1850 - val_accuracy: 0.7006\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.2788 - accuracy: 0.7649 - val_loss: 3.2099 - val_accuracy: 0.7074\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 1.2566 - accuracy: 0.7664 - val_loss: 3.2156 - val_accuracy: 0.7039\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.2318 - accuracy: 0.7694 - val_loss: 3.2325 - val_accuracy: 0.7022\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.2134 - accuracy: 0.7715 - val_loss: 3.2560 - val_accuracy: 0.7064\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.1978 - accuracy: 0.7726 - val_loss: 3.2706 - val_accuracy: 0.6995\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.1797 - accuracy: 0.7741 - val_loss: 3.2665 - val_accuracy: 0.7060\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.1563 - accuracy: 0.7775 - val_loss: 3.2816 - val_accuracy: 0.7022\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.1322 - accuracy: 0.7804 - val_loss: 3.2963 - val_accuracy: 0.7036\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.1088 - accuracy: 0.7836 - val_loss: 3.3112 - val_accuracy: 0.7053\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.0896 - accuracy: 0.7846 - val_loss: 3.3263 - val_accuracy: 0.7021\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 1.0710 - accuracy: 0.7881 - val_loss: 3.3383 - val_accuracy: 0.7037\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 1.0478 - accuracy: 0.7910 - val_loss: 3.3689 - val_accuracy: 0.7032\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 1.0266 - accuracy: 0.7931 - val_loss: 3.3486 - val_accuracy: 0.7028\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 1.0048 - accuracy: 0.7968 - val_loss: 3.3912 - val_accuracy: 0.7034\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.9817 - accuracy: 0.8004 - val_loss: 3.3904 - val_accuracy: 0.7041\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.9574 - accuracy: 0.8036 - val_loss: 3.4141 - val_accuracy: 0.7029\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.9402 - accuracy: 0.8055 - val_loss: 3.4215 - val_accuracy: 0.7034\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.9193 - accuracy: 0.8090 - val_loss: 3.4501 - val_accuracy: 0.7031\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.8938 - accuracy: 0.8130 - val_loss: 3.4578 - val_accuracy: 0.7025\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.8720 - accuracy: 0.8150 - val_loss: 3.4779 - val_accuracy: 0.7019\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.8521 - accuracy: 0.8196 - val_loss: 3.4832 - val_accuracy: 0.7007\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.8292 - accuracy: 0.8227 - val_loss: 3.5029 - val_accuracy: 0.7022\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 1s 204ms/step - loss: 0.8132 - accuracy: 0.8258 - val_loss: 3.5063 - val_accuracy: 0.7024\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.7897 - accuracy: 0.8295 - val_loss: 3.5386 - val_accuracy: 0.7006\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.7713 - accuracy: 0.8324 - val_loss: 3.5393 - val_accuracy: 0.6993\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.7632 - accuracy: 0.8335 - val_loss: 3.5545 - val_accuracy: 0.6993\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.7438 - accuracy: 0.8357 - val_loss: 3.5501 - val_accuracy: 0.7026\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.7266 - accuracy: 0.8389 - val_loss: 3.5701 - val_accuracy: 0.7023\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.7066 - accuracy: 0.8419 - val_loss: 3.5835 - val_accuracy: 0.6960\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.7083 - accuracy: 0.8413 - val_loss: 3.5843 - val_accuracy: 0.7020\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.6748 - accuracy: 0.8482 - val_loss: 3.5771 - val_accuracy: 0.7033\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.6663 - accuracy: 0.8492 - val_loss: 3.5977 - val_accuracy: 0.6954\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.6680 - accuracy: 0.8483 - val_loss: 3.5939 - val_accuracy: 0.7042\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.6515 - accuracy: 0.8508 - val_loss: 3.5930 - val_accuracy: 0.7005\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.6327 - accuracy: 0.8557 - val_loss: 3.5922 - val_accuracy: 0.7000\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.6115 - accuracy: 0.8593 - val_loss: 3.5997 - val_accuracy: 0.7010\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.5971 - accuracy: 0.8617 - val_loss: 3.6084 - val_accuracy: 0.7011\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.5821 - accuracy: 0.8639 - val_loss: 3.6095 - val_accuracy: 0.7024\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.5693 - accuracy: 0.8671 - val_loss: 3.6203 - val_accuracy: 0.7001\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.5459 - accuracy: 0.8712 - val_loss: 3.6158 - val_accuracy: 0.7031\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.5386 - accuracy: 0.8729 - val_loss: 3.6435 - val_accuracy: 0.7009\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.5276 - accuracy: 0.8751 - val_loss: 3.6454 - val_accuracy: 0.7016\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.5098 - accuracy: 0.8783 - val_loss: 3.6584 - val_accuracy: 0.7005\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.4932 - accuracy: 0.8832 - val_loss: 3.6831 - val_accuracy: 0.7016\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.4806 - accuracy: 0.8832 - val_loss: 3.6888 - val_accuracy: 0.7008\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.4672 - accuracy: 0.8868 - val_loss: 3.7033 - val_accuracy: 0.7004\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 0.4545 - accuracy: 0.8896 - val_loss: 3.7017 - val_accuracy: 0.7021\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.4444 - accuracy: 0.8918 - val_loss: 3.7247 - val_accuracy: 0.7009\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.4340 - accuracy: 0.8934 - val_loss: 3.7380 - val_accuracy: 0.6993\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.4215 - accuracy: 0.8954 - val_loss: 3.7363 - val_accuracy: 0.7019\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.4124 - accuracy: 0.8964 - val_loss: 3.7482 - val_accuracy: 0.7000\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.4052 - accuracy: 0.8992 - val_loss: 3.7577 - val_accuracy: 0.6991\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.3927 - accuracy: 0.9037 - val_loss: 3.7611 - val_accuracy: 0.7021\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.3884 - accuracy: 0.9031 - val_loss: 3.7695 - val_accuracy: 0.7007\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.3748 - accuracy: 0.9052 - val_loss: 3.7886 - val_accuracy: 0.6995\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.3736 - accuracy: 0.9048 - val_loss: 3.7766 - val_accuracy: 0.7013\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.3636 - accuracy: 0.9080 - val_loss: 3.7918 - val_accuracy: 0.7001\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.3597 - accuracy: 0.9082 - val_loss: 3.8062 - val_accuracy: 0.6982\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.3554 - accuracy: 0.9095 - val_loss: 3.7998 - val_accuracy: 0.7015\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.3493 - accuracy: 0.9100 - val_loss: 3.7976 - val_accuracy: 0.6999\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.3330 - accuracy: 0.9140 - val_loss: 3.8106 - val_accuracy: 0.6989\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.3320 - accuracy: 0.9149 - val_loss: 3.8126 - val_accuracy: 0.7024\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.3279 - accuracy: 0.9155 - val_loss: 3.8307 - val_accuracy: 0.6983\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.3238 - accuracy: 0.9162 - val_loss: 3.8325 - val_accuracy: 0.7020\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.3094 - accuracy: 0.9189 - val_loss: 3.8369 - val_accuracy: 0.7009\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.3030 - accuracy: 0.9205 - val_loss: 3.8620 - val_accuracy: 0.6991\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2970 - accuracy: 0.9220 - val_loss: 3.8497 - val_accuracy: 0.7026\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2946 - accuracy: 0.9219 - val_loss: 3.8554 - val_accuracy: 0.6993\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2857 - accuracy: 0.9247 - val_loss: 3.8740 - val_accuracy: 0.6998\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2762 - accuracy: 0.9270 - val_loss: 3.8662 - val_accuracy: 0.7017\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2745 - accuracy: 0.9271 - val_loss: 3.8771 - val_accuracy: 0.6997\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2673 - accuracy: 0.9292 - val_loss: 3.8756 - val_accuracy: 0.7016\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2630 - accuracy: 0.9294 - val_loss: 3.8880 - val_accuracy: 0.7003\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2537 - accuracy: 0.9331 - val_loss: 3.9087 - val_accuracy: 0.6981\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 1s 202ms/step - loss: 0.2535 - accuracy: 0.9316 - val_loss: 3.9008 - val_accuracy: 0.7015\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2472 - accuracy: 0.9336 - val_loss: 3.9097 - val_accuracy: 0.7007\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2374 - accuracy: 0.9365 - val_loss: 3.9161 - val_accuracy: 0.7008\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.2366 - accuracy: 0.9362 - val_loss: 3.9100 - val_accuracy: 0.7017\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2343 - accuracy: 0.9368 - val_loss: 3.9242 - val_accuracy: 0.6995\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.2291 - accuracy: 0.9380 - val_loss: 3.9360 - val_accuracy: 0.7008\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.2227 - accuracy: 0.9399 - val_loss: 3.9427 - val_accuracy: 0.7001\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2201 - accuracy: 0.9411 - val_loss: 3.9455 - val_accuracy: 0.6984\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.2188 - accuracy: 0.9406 - val_loss: 3.9463 - val_accuracy: 0.7023\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.2152 - accuracy: 0.9401 - val_loss: 3.9642 - val_accuracy: 0.6997\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2103 - accuracy: 0.9421 - val_loss: 3.9434 - val_accuracy: 0.7010\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.2067 - accuracy: 0.9432 - val_loss: 3.9453 - val_accuracy: 0.7005\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2055 - accuracy: 0.9436 - val_loss: 3.9548 - val_accuracy: 0.7009\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.2003 - accuracy: 0.9451 - val_loss: 3.9618 - val_accuracy: 0.7013\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1959 - accuracy: 0.9460 - val_loss: 3.9690 - val_accuracy: 0.6997\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1940 - accuracy: 0.9467 - val_loss: 3.9710 - val_accuracy: 0.7007\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1882 - accuracy: 0.9484 - val_loss: 3.9764 - val_accuracy: 0.7013\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.1855 - accuracy: 0.9487 - val_loss: 3.9754 - val_accuracy: 0.7006\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1838 - accuracy: 0.9491 - val_loss: 3.9830 - val_accuracy: 0.7005\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1786 - accuracy: 0.9514 - val_loss: 3.9898 - val_accuracy: 0.7009\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1760 - accuracy: 0.9513 - val_loss: 3.9986 - val_accuracy: 0.7014\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1755 - accuracy: 0.9511 - val_loss: 4.0224 - val_accuracy: 0.6986\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1758 - accuracy: 0.9511 - val_loss: 3.9966 - val_accuracy: 0.7011\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1695 - accuracy: 0.9529 - val_loss: 3.9976 - val_accuracy: 0.7000\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1659 - accuracy: 0.9537 - val_loss: 4.0249 - val_accuracy: 0.6977\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1672 - accuracy: 0.9531 - val_loss: 4.0090 - val_accuracy: 0.7008\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1655 - accuracy: 0.9539 - val_loss: 4.0166 - val_accuracy: 0.6978\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.1610 - accuracy: 0.9555 - val_loss: 4.0204 - val_accuracy: 0.7024\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1675 - accuracy: 0.9532 - val_loss: 4.0367 - val_accuracy: 0.6991\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1605 - accuracy: 0.9549 - val_loss: 4.0279 - val_accuracy: 0.6988\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1602 - accuracy: 0.9548 - val_loss: 4.0149 - val_accuracy: 0.7022\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1616 - accuracy: 0.9541 - val_loss: 4.0426 - val_accuracy: 0.6979\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1602 - accuracy: 0.9544 - val_loss: 4.0341 - val_accuracy: 0.7016\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1567 - accuracy: 0.9550 - val_loss: 4.0413 - val_accuracy: 0.6985\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.1577 - accuracy: 0.9554 - val_loss: 4.0253 - val_accuracy: 0.7015\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 1s 201ms/step - loss: 0.1506 - accuracy: 0.9571 - val_loss: 4.0519 - val_accuracy: 0.6986\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1473 - accuracy: 0.9582 - val_loss: 4.0324 - val_accuracy: 0.7024\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1439 - accuracy: 0.9584 - val_loss: 4.0548 - val_accuracy: 0.6990\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1423 - accuracy: 0.9590 - val_loss: 4.0564 - val_accuracy: 0.7022\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1422 - accuracy: 0.9590 - val_loss: 4.0711 - val_accuracy: 0.6979\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1402 - accuracy: 0.9596 - val_loss: 4.0541 - val_accuracy: 0.7020\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1374 - accuracy: 0.9602 - val_loss: 4.0745 - val_accuracy: 0.6987\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1352 - accuracy: 0.9607 - val_loss: 4.0582 - val_accuracy: 0.7017\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1333 - accuracy: 0.9613 - val_loss: 4.0789 - val_accuracy: 0.6971\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 1s 203ms/step - loss: 0.1312 - accuracy: 0.9623 - val_loss: 4.0616 - val_accuracy: 0.7014\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1315 - accuracy: 0.9618 - val_loss: 4.0751 - val_accuracy: 0.6970\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1334 - accuracy: 0.9613 - val_loss: 4.0603 - val_accuracy: 0.7021\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1339 - accuracy: 0.9609 - val_loss: 4.0901 - val_accuracy: 0.6970\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1256 - accuracy: 0.9634 - val_loss: 4.0739 - val_accuracy: 0.7017\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1324 - accuracy: 0.9620 - val_loss: 4.0938 - val_accuracy: 0.6968\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1274 - accuracy: 0.9628 - val_loss: 4.0815 - val_accuracy: 0.7016\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1256 - accuracy: 0.9639 - val_loss: 4.1034 - val_accuracy: 0.6974\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1249 - accuracy: 0.9642 - val_loss: 4.0824 - val_accuracy: 0.7016\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1220 - accuracy: 0.9637 - val_loss: 4.1066 - val_accuracy: 0.6976\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1177 - accuracy: 0.9655 - val_loss: 4.0885 - val_accuracy: 0.7017\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1125 - accuracy: 0.9676 - val_loss: 4.1052 - val_accuracy: 0.6974\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1124 - accuracy: 0.9680 - val_loss: 4.1030 - val_accuracy: 0.7006\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.1110 - accuracy: 0.9675 - val_loss: 4.1132 - val_accuracy: 0.6991\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1095 - accuracy: 0.9678 - val_loss: 4.1163 - val_accuracy: 0.6996\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1060 - accuracy: 0.9695 - val_loss: 4.1186 - val_accuracy: 0.6992\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1058 - accuracy: 0.9695 - val_loss: 4.1224 - val_accuracy: 0.6997\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.1018 - accuracy: 0.9706 - val_loss: 4.1355 - val_accuracy: 0.6994\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 1s 199ms/step - loss: 0.1011 - accuracy: 0.9712 - val_loss: 4.1332 - val_accuracy: 0.7001\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.0991 - accuracy: 0.9712 - val_loss: 4.1401 - val_accuracy: 0.6995\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 0.0995 - accuracy: 0.9712 - val_loss: 4.1439 - val_accuracy: 0.6993\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 1s 198ms/step - loss: 0.0998 - accuracy: 0.9709 - val_loss: 4.1392 - val_accuracy: 0.7001\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 1s 197ms/step - loss: 0.0963 - accuracy: 0.9718 - val_loss: 4.1414 - val_accuracy: 0.6989\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f87126a7680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "u waye ngu sho fa . [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhdEiWgvNJhA"
      },
      "source": [
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a bidirectional RNN model on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.003\n",
        "    \n",
        "    # TODO: Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
        "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1pv2ISeu05x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0347eec3-0465-4fb9-9621-9893f9c3f215"
      },
      "source": [
        "#tests.test_bd_model(bd_model)\n",
        "\n",
        "# TODO: Reshape the input\n",
        "tmp_x = pad(preproc_english_sentences, preproc_zulu_sentences.shape[1])\n",
        "tmp_x = tmp_x.reshape((-1, preproc_zulu_sentences.shape[-2]))\n",
        "\n",
        "# TODO: Train and Print prediction(s)\n",
        "embed_rnn_model1 = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_zulu_sentences.shape[1],\n",
        "    english_vocab_size+1,\n",
        "    zulu_vocab_size+1)\n",
        "\n",
        "embed_rnn_model1.summary()\n",
        "\n",
        "embed_rnn_model1.fit(tmp_x, preproc_zulu_sentences, batch_size=1024, epochs=50, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text(embed_rnn_model1.predict(tmp_x[:1])[0], zulu_tokenizer))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 22, 256)           1315072   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 22, 256)          296448    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed_10 (TimeDi  (None, 22, 1024)         263168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 22, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_11 (TimeDi  (None, 22, 7841)         8037025   \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,911,713\n",
            "Trainable params: 9,911,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "2/2 [==============================] - 4s 755ms/step - loss: 8.9432 - accuracy: 0.1141 - val_loss: 8.5489 - val_accuracy: 0.6001\n",
            "Epoch 2/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 8.4023 - accuracy: 0.6289 - val_loss: 6.1396 - val_accuracy: 0.6001\n",
            "Epoch 3/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 5.6161 - accuracy: 0.6289 - val_loss: 5.7434 - val_accuracy: 0.6001\n",
            "Epoch 4/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 5.1329 - accuracy: 0.6289 - val_loss: 4.7743 - val_accuracy: 0.6001\n",
            "Epoch 5/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 3.9256 - accuracy: 0.6289 - val_loss: 4.1313 - val_accuracy: 0.6001\n",
            "Epoch 6/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 3.4068 - accuracy: 0.6289 - val_loss: 4.2266 - val_accuracy: 0.6002\n",
            "Epoch 7/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 3.6159 - accuracy: 0.6298 - val_loss: 4.2392 - val_accuracy: 0.6003\n",
            "Epoch 8/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 3.5293 - accuracy: 0.6303 - val_loss: 4.3331 - val_accuracy: 0.6003\n",
            "Epoch 9/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 3.3711 - accuracy: 0.6301 - val_loss: 4.5959 - val_accuracy: 0.6003\n",
            "Epoch 10/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 3.4519 - accuracy: 0.6303 - val_loss: 4.5849 - val_accuracy: 0.6003\n",
            "Epoch 11/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 3.3449 - accuracy: 0.6301 - val_loss: 4.4613 - val_accuracy: 0.6003\n",
            "Epoch 12/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 3.2500 - accuracy: 0.6306 - val_loss: 4.4318 - val_accuracy: 0.6001\n",
            "Epoch 13/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 3.2496 - accuracy: 0.6307 - val_loss: 4.4549 - val_accuracy: 0.5998\n",
            "Epoch 14/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 3.2129 - accuracy: 0.6307 - val_loss: 4.5132 - val_accuracy: 0.5999\n",
            "Epoch 15/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 3.1467 - accuracy: 0.6308 - val_loss: 4.6061 - val_accuracy: 0.6001\n",
            "Epoch 16/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 3.1125 - accuracy: 0.6306 - val_loss: 4.6772 - val_accuracy: 0.6001\n",
            "Epoch 17/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 3.1029 - accuracy: 0.6304 - val_loss: 4.6786 - val_accuracy: 0.6001\n",
            "Epoch 18/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 3.0667 - accuracy: 0.6303 - val_loss: 4.6465 - val_accuracy: 0.5996\n",
            "Epoch 19/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 3.0335 - accuracy: 0.6306 - val_loss: 4.6304 - val_accuracy: 0.5989\n",
            "Epoch 20/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 3.0165 - accuracy: 0.6308 - val_loss: 4.6414 - val_accuracy: 0.5989\n",
            "Epoch 21/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.9937 - accuracy: 0.6312 - val_loss: 4.6753 - val_accuracy: 0.5993\n",
            "Epoch 22/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.9639 - accuracy: 0.6309 - val_loss: 4.7202 - val_accuracy: 0.5996\n",
            "Epoch 23/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.9405 - accuracy: 0.6312 - val_loss: 4.7545 - val_accuracy: 0.5996\n",
            "Epoch 24/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 2.9174 - accuracy: 0.6309 - val_loss: 4.7669 - val_accuracy: 0.5992\n",
            "Epoch 25/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 2.8884 - accuracy: 0.6312 - val_loss: 4.7713 - val_accuracy: 0.5989\n",
            "Epoch 26/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.8623 - accuracy: 0.6312 - val_loss: 4.7859 - val_accuracy: 0.5990\n",
            "Epoch 27/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.8400 - accuracy: 0.6316 - val_loss: 4.8176 - val_accuracy: 0.5988\n",
            "Epoch 28/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.8131 - accuracy: 0.6319 - val_loss: 4.8643 - val_accuracy: 0.5989\n",
            "Epoch 29/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.7840 - accuracy: 0.6316 - val_loss: 4.9142 - val_accuracy: 0.5990\n",
            "Epoch 30/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.7597 - accuracy: 0.6321 - val_loss: 4.9513 - val_accuracy: 0.5988\n",
            "Epoch 31/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.7326 - accuracy: 0.6317 - val_loss: 4.9780 - val_accuracy: 0.5980\n",
            "Epoch 32/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.7054 - accuracy: 0.6323 - val_loss: 5.0050 - val_accuracy: 0.5979\n",
            "Epoch 33/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.6766 - accuracy: 0.6325 - val_loss: 5.0368 - val_accuracy: 0.5980\n",
            "Epoch 34/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.6501 - accuracy: 0.6326 - val_loss: 5.0712 - val_accuracy: 0.5979\n",
            "Epoch 35/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.6214 - accuracy: 0.6333 - val_loss: 5.1042 - val_accuracy: 0.5980\n",
            "Epoch 36/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.5922 - accuracy: 0.6334 - val_loss: 5.1356 - val_accuracy: 0.5985\n",
            "Epoch 37/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.5624 - accuracy: 0.6342 - val_loss: 5.1650 - val_accuracy: 0.5985\n",
            "Epoch 38/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 2.5274 - accuracy: 0.6345 - val_loss: 5.1935 - val_accuracy: 0.5979\n",
            "Epoch 39/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.4969 - accuracy: 0.6352 - val_loss: 5.2206 - val_accuracy: 0.5979\n",
            "Epoch 40/50\n",
            "2/2 [==============================] - 0s 133ms/step - loss: 2.4685 - accuracy: 0.6348 - val_loss: 5.2485 - val_accuracy: 0.5977\n",
            "Epoch 41/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.4410 - accuracy: 0.6353 - val_loss: 5.2760 - val_accuracy: 0.5982\n",
            "Epoch 42/50\n",
            "2/2 [==============================] - 0s 134ms/step - loss: 2.4111 - accuracy: 0.6360 - val_loss: 5.3017 - val_accuracy: 0.5983\n",
            "Epoch 43/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.3781 - accuracy: 0.6375 - val_loss: 5.3269 - val_accuracy: 0.5972\n",
            "Epoch 44/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 2.3476 - accuracy: 0.6377 - val_loss: 5.3519 - val_accuracy: 0.5966\n",
            "Epoch 45/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.3192 - accuracy: 0.6391 - val_loss: 5.3725 - val_accuracy: 0.5969\n",
            "Epoch 46/50\n",
            "2/2 [==============================] - 0s 137ms/step - loss: 2.2932 - accuracy: 0.6393 - val_loss: 5.3895 - val_accuracy: 0.5963\n",
            "Epoch 47/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.2600 - accuracy: 0.6400 - val_loss: 5.4081 - val_accuracy: 0.5958\n",
            "Epoch 48/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.2353 - accuracy: 0.6405 - val_loss: 5.4267 - val_accuracy: 0.5954\n",
            "Epoch 49/50\n",
            "2/2 [==============================] - 0s 135ms/step - loss: 2.2075 - accuracy: 0.6403 - val_loss: 5.4423 - val_accuracy: 0.5944\n",
            "Epoch 50/50\n",
            "2/2 [==============================] - 0s 136ms/step - loss: 2.1776 - accuracy: 0.6434 - val_loss: 5.4574 - val_accuracy: 0.5950\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8712507dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[6.22691493e-03 5.91909420e-03 3.47483531e-03 ... 6.61500854e-09\n",
            "  6.45407949e-09 5.58684565e-09]\n",
            " [1.04583092e-01 1.22351618e-03 1.66932622e-03 ... 1.91975769e-09\n",
            "  1.72915093e-09 1.29450617e-09]\n",
            " [5.34793794e-01 7.01146666e-04 1.01684080e-03 ... 2.33475999e-12\n",
            "  1.89357926e-12 1.34969954e-12]\n",
            " ...\n",
            " [9.99979854e-01 1.48725010e-09 2.02421813e-08 ... 4.32239748e-35\n",
            "  1.17599356e-35 7.74218505e-36]\n",
            " [9.99963999e-01 1.09418066e-08 1.03116953e-07 ... 5.11005578e-34\n",
            "  1.57199720e-34 1.11535094e-34]\n",
            " [9.99623060e-01 2.04820958e-06 7.41320537e-06 ... 1.97819031e-31\n",
            "  7.80627167e-32 5.91527448e-32]] (22, 7841)\n",
            "[40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "2019 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04xhuwvSu4Gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6b2565-773a-49d8-89f0-444368c9079b"
      },
      "source": [
        "tmp_x_bpe = pad(preproc_english_sentences_bpe, preproc_zulu_sentences_bpe.shape[1])\n",
        "tmp_x_bpe = tmp_x_bpe.reshape((-1, preproc_zulu_sentences_bpe.shape[-2]))\n",
        "\n",
        "# TODO: Train and Print prediction(s)\n",
        "embed_rnn_model1_bpe = bd_model(\n",
        "    tmp_x_bpe.shape,\n",
        "    preproc_zulu_sentences_bpe.shape[1],\n",
        "    english_vocab_size_bpe+1,\n",
        "    zulu_vocab_size_bpe+1)\n",
        "\n",
        "embed_rnn_model1_bpe.summary()\n",
        "\n",
        "embed_rnn_model1_bpe.fit(tmp_x_bpe, preproc_zulu_sentences_bpe, batch_size=1024, epochs=20, validation_split=0.2)\n",
        "\n",
        "print(logits_to_text_bpe(embed_rnn_model1_bpe.predict(tmp_x_bpe[:1])[0], tokenizer_zu))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 58, 256)           2560256   \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 58, 256)          296448    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_28 (TimeDi  (None, 58, 1024)         263168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 58, 1024)          0         \n",
            "                                                                 \n",
            " time_distributed_29 (TimeDi  (None, 58, 10001)        10251025  \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,370,897\n",
            "Trainable params: 13,370,897\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "2/2 [==============================] - 5s 1s/step - loss: 9.1641 - accuracy: 0.2971 - val_loss: 8.7728 - val_accuracy: 0.6699\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 2s 790ms/step - loss: 8.4143 - accuracy: 0.6807 - val_loss: 6.0247 - val_accuracy: 0.6699\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 2s 789ms/step - loss: 4.7087 - accuracy: 0.6807 - val_loss: 3.7364 - val_accuracy: 0.6699\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 2s 791ms/step - loss: 3.4989 - accuracy: 0.6807 - val_loss: 3.6459 - val_accuracy: 0.6699\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 2s 793ms/step - loss: 3.0933 - accuracy: 0.6807 - val_loss: 3.2718 - val_accuracy: 0.6699\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 2s 791ms/step - loss: 2.8986 - accuracy: 0.6809 - val_loss: 3.3008 - val_accuracy: 0.6711\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 2s 791ms/step - loss: 2.9633 - accuracy: 0.6827 - val_loss: 3.2626 - val_accuracy: 0.6712\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 2s 792ms/step - loss: 2.9005 - accuracy: 0.6819 - val_loss: 3.1664 - val_accuracy: 0.6701\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 2s 790ms/step - loss: 2.7913 - accuracy: 0.6809 - val_loss: 3.1730 - val_accuracy: 0.6700\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 2s 790ms/step - loss: 2.7758 - accuracy: 0.6808 - val_loss: 3.2261 - val_accuracy: 0.6699\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 2s 789ms/step - loss: 2.7725 - accuracy: 0.6808 - val_loss: 3.2194 - val_accuracy: 0.6700\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 2s 792ms/step - loss: 2.7248 - accuracy: 0.6808 - val_loss: 3.1804 - val_accuracy: 0.6703\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 2s 793ms/step - loss: 2.6894 - accuracy: 0.6809 - val_loss: 3.1607 - val_accuracy: 0.6703\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 2s 791ms/step - loss: 2.6678 - accuracy: 0.6809 - val_loss: 3.1471 - val_accuracy: 0.6702\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 2s 791ms/step - loss: 2.6533 - accuracy: 0.6809 - val_loss: 3.1221 - val_accuracy: 0.6700\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 2s 790ms/step - loss: 2.6362 - accuracy: 0.6809 - val_loss: 3.0861 - val_accuracy: 0.6702\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 2s 791ms/step - loss: 2.6132 - accuracy: 0.6809 - val_loss: 3.0592 - val_accuracy: 0.6701\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 2s 795ms/step - loss: 2.5921 - accuracy: 0.6810 - val_loss: 3.0431 - val_accuracy: 0.6706\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 2s 790ms/step - loss: 2.5688 - accuracy: 0.6811 - val_loss: 3.0291 - val_accuracy: 0.6707\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 2s 795ms/step - loss: 2.5495 - accuracy: 0.6812 - val_loss: 3.0027 - val_accuracy: 0.6710\n",
            "- [UNK] [UNK] [UNK] [UNK] [UNK] - [UNK] [UNK] [UNK] [UNK] [UNK] , [UNK] - - [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT8_B8rkvYhX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vkddtFqvYjg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94ZsvHfWvYmk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5-CCBpaSj5Z",
        "outputId": "69776d21-cb24-482c-fed8-2199f7ac8942"
      },
      "source": [
        "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "    \"\"\"\n",
        "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
        "    :param input_shape: Tuple of input shape\n",
        "    :param output_sequence_length: Length of output sequence\n",
        "    :param english_vocab_size: Number of unique English words in the dataset\n",
        "    :param french_vocab_size: Number of unique French words in the dataset\n",
        "    :return: Keras model built, but not trained\n",
        "    \"\"\"\n",
        "    # TODO: Implement\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.005\n",
        "    \n",
        "    # Build the layers    \n",
        "    model = Sequential()\n",
        "    # Embedding\n",
        "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
        "                         input_shape=input_shape[1:]))\n",
        "    # Encoder\n",
        "    model.add(Bidirectional(GRU(128)))\n",
        "    model.add(RepeatVector(output_sequence_length))\n",
        "    # Decoder\n",
        "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "#tests.test_model_final(model_final)\n",
        "\n",
        "print('Final Model Loaded')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBPleeftSwS7",
        "outputId": "22d53651-9dea-454e-eaab-15065b84206d"
      },
      "source": [
        "def final_predictions(x, y, x_tk, y_tk):\n",
        "    \"\"\"\n",
        "    Gets predictions using the final model\n",
        "    :param x: Preprocessed English data\n",
        "    :param y: Preprocessed French data\n",
        "    :param x_tk: English tokenizer\n",
        "    :param y_tk: French tokenizer\n",
        "    \"\"\"\n",
        "    # TODO: Train neural network using model_final\n",
        "    model = model_final(x.shape,y.shape[1],\n",
        "                        len(x_tk.word_index)+1,\n",
        "                        len(y_tk.word_index)+1)\n",
        "    model.summary()\n",
        "    model.fit(x, y, batch_size=1024, epochs=200, validation_split=0.2)\n",
        "\n",
        "    \n",
        "    print(logits_to_text(model.predict(x[:1])[0], zulu_tokenizer))\n",
        "    \n",
        "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
        "    # y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
        "    # y_id_to_word[0] = '<PAD>'\n",
        "\n",
        "    # sentence = 'he saw a old yellow truck'\n",
        "    # sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
        "    # sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
        "    # sentences = np.array([sentence[0], x[0]])\n",
        "    # predictions = model.predict(sentences, len(sentences))\n",
        "\n",
        "    # print('Sample 1:')\n",
        "    # print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
        "    # print('Il a vu un vieux camion jaune')\n",
        "    # print('Sample 2:')\n",
        "    # print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
        "    # print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
        "\n",
        "\n",
        "final_predictions(preproc_english_sentences, preproc_zulu_sentences, english_tokenizer, zulu_tokenizer)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 19, 128)           657536    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 256)              198144    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 22, 256)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 22, 256)          296448    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_14 (TimeDi  (None, 22, 512)          131584    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 22, 512)           0         \n",
            "                                                                 \n",
            " time_distributed_15 (TimeDi  (None, 22, 7841)         4022433   \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,306,145\n",
            "Trainable params: 5,306,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "2/2 [==============================] - 6s 1s/step - loss: 8.9485 - accuracy: 0.1133 - val_loss: 7.9787 - val_accuracy: 0.6001\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 7.2459 - accuracy: 0.6289 - val_loss: 6.3139 - val_accuracy: 0.6001\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 5.8496 - accuracy: 0.6289 - val_loss: 6.3891 - val_accuracy: 0.6001\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 5.9046 - accuracy: 0.6289 - val_loss: 6.2528 - val_accuracy: 0.6001\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 5.8178 - accuracy: 0.6249 - val_loss: 6.1732 - val_accuracy: 0.6001\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 5.7284 - accuracy: 0.6289 - val_loss: 6.1542 - val_accuracy: 0.6001\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 5.7131 - accuracy: 0.6289 - val_loss: 6.1461 - val_accuracy: 0.6001\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 5.6922 - accuracy: 0.6289 - val_loss: 6.1175 - val_accuracy: 0.6001\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 5.6519 - accuracy: 0.6289 - val_loss: 6.0468 - val_accuracy: 0.6001\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 5.5817 - accuracy: 0.6289 - val_loss: 5.9249 - val_accuracy: 0.6001\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 5.4101 - accuracy: 0.6289 - val_loss: 5.3407 - val_accuracy: 0.6001\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 4.6086 - accuracy: 0.6289 - val_loss: 4.2572 - val_accuracy: 0.6001\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 3.7620 - accuracy: 0.6289 - val_loss: 4.0069 - val_accuracy: 0.6001\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.6181 - accuracy: 0.6289 - val_loss: 4.2046 - val_accuracy: 0.6001\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.6213 - accuracy: 0.6289 - val_loss: 3.9978 - val_accuracy: 0.6001\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.5440 - accuracy: 0.6289 - val_loss: 4.2338 - val_accuracy: 0.6001\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.5193 - accuracy: 0.6289 - val_loss: 4.1471 - val_accuracy: 0.6001\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.5044 - accuracy: 0.6289 - val_loss: 4.3859 - val_accuracy: 0.6001\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 3.4967 - accuracy: 0.6289 - val_loss: 4.3043 - val_accuracy: 0.6001\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.4614 - accuracy: 0.6289 - val_loss: 4.4068 - val_accuracy: 0.6001\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.4431 - accuracy: 0.6289 - val_loss: 4.3216 - val_accuracy: 0.6001\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.4150 - accuracy: 0.6289 - val_loss: 4.4243 - val_accuracy: 0.6001\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 3.4031 - accuracy: 0.6289 - val_loss: 4.2731 - val_accuracy: 0.6001\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.3990 - accuracy: 0.6289 - val_loss: 4.4241 - val_accuracy: 0.6001\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 3.3468 - accuracy: 0.6289 - val_loss: 4.3092 - val_accuracy: 0.6001\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.3178 - accuracy: 0.6289 - val_loss: 4.4805 - val_accuracy: 0.6001\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.3015 - accuracy: 0.6289 - val_loss: 4.4447 - val_accuracy: 0.6001\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.2734 - accuracy: 0.6289 - val_loss: 4.3932 - val_accuracy: 0.6001\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.2684 - accuracy: 0.6291 - val_loss: 4.6009 - val_accuracy: 0.6001\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.2579 - accuracy: 0.6292 - val_loss: 4.4061 - val_accuracy: 0.6001\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.2502 - accuracy: 0.6296 - val_loss: 4.5853 - val_accuracy: 0.6001\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.2182 - accuracy: 0.6299 - val_loss: 4.4937 - val_accuracy: 0.6019\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.1951 - accuracy: 0.6306 - val_loss: 4.4782 - val_accuracy: 0.6025\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.1865 - accuracy: 0.6312 - val_loss: 4.6221 - val_accuracy: 0.6024\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.1831 - accuracy: 0.6312 - val_loss: 4.4849 - val_accuracy: 0.6025\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 3.1873 - accuracy: 0.6319 - val_loss: 4.6685 - val_accuracy: 0.6027\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.1631 - accuracy: 0.6317 - val_loss: 4.5810 - val_accuracy: 0.6028\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.1418 - accuracy: 0.6323 - val_loss: 4.5639 - val_accuracy: 0.6028\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.1317 - accuracy: 0.6323 - val_loss: 4.7105 - val_accuracy: 0.6027\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.1374 - accuracy: 0.6320 - val_loss: 4.5326 - val_accuracy: 0.6024\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 3.1378 - accuracy: 0.6323 - val_loss: 4.7459 - val_accuracy: 0.6027\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.1254 - accuracy: 0.6324 - val_loss: 4.6129 - val_accuracy: 0.6024\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.0983 - accuracy: 0.6326 - val_loss: 4.6263 - val_accuracy: 0.6027\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.0840 - accuracy: 0.6327 - val_loss: 4.7571 - val_accuracy: 0.6028\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.1006 - accuracy: 0.6325 - val_loss: 4.5992 - val_accuracy: 0.6014\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.0931 - accuracy: 0.6325 - val_loss: 4.7541 - val_accuracy: 0.6027\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 3.0705 - accuracy: 0.6326 - val_loss: 4.6835 - val_accuracy: 0.6018\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.0544 - accuracy: 0.6327 - val_loss: 4.6357 - val_accuracy: 0.6012\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.0535 - accuracy: 0.6326 - val_loss: 4.7909 - val_accuracy: 0.6030\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.0684 - accuracy: 0.6328 - val_loss: 4.6803 - val_accuracy: 0.6018\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.0352 - accuracy: 0.6331 - val_loss: 4.6847 - val_accuracy: 0.6009\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.0282 - accuracy: 0.6328 - val_loss: 4.8192 - val_accuracy: 0.6027\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 3.0417 - accuracy: 0.6326 - val_loss: 4.7150 - val_accuracy: 0.6018\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.0172 - accuracy: 0.6332 - val_loss: 4.6980 - val_accuracy: 0.6011\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.0216 - accuracy: 0.6332 - val_loss: 4.8305 - val_accuracy: 0.6018\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 3.0206 - accuracy: 0.6333 - val_loss: 4.7984 - val_accuracy: 0.6017\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9987 - accuracy: 0.6341 - val_loss: 4.7256 - val_accuracy: 0.5999\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 3.0199 - accuracy: 0.6333 - val_loss: 4.7690 - val_accuracy: 0.6008\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9878 - accuracy: 0.6340 - val_loss: 4.8493 - val_accuracy: 0.6018\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 3.0064 - accuracy: 0.6336 - val_loss: 4.8026 - val_accuracy: 0.6006\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9832 - accuracy: 0.6341 - val_loss: 4.7851 - val_accuracy: 0.6001\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9840 - accuracy: 0.6339 - val_loss: 4.8150 - val_accuracy: 0.6006\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9699 - accuracy: 0.6340 - val_loss: 4.8466 - val_accuracy: 0.6015\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.9688 - accuracy: 0.6342 - val_loss: 4.8554 - val_accuracy: 0.6006\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9599 - accuracy: 0.6338 - val_loss: 4.8520 - val_accuracy: 0.5999\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9565 - accuracy: 0.6335 - val_loss: 4.8276 - val_accuracy: 0.6001\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9571 - accuracy: 0.6341 - val_loss: 4.8052 - val_accuracy: 0.5998\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9825 - accuracy: 0.6342 - val_loss: 4.8293 - val_accuracy: 0.5986\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9904 - accuracy: 0.6337 - val_loss: 4.9160 - val_accuracy: 0.6006\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9491 - accuracy: 0.6329 - val_loss: 4.9874 - val_accuracy: 0.6021\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9806 - accuracy: 0.6337 - val_loss: 4.9006 - val_accuracy: 0.6008\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9480 - accuracy: 0.6336 - val_loss: 4.8516 - val_accuracy: 0.5982\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9788 - accuracy: 0.6338 - val_loss: 4.9307 - val_accuracy: 0.6008\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 2.9432 - accuracy: 0.6337 - val_loss: 4.9726 - val_accuracy: 0.6015\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9450 - accuracy: 0.6337 - val_loss: 4.9206 - val_accuracy: 0.6005\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9368 - accuracy: 0.6342 - val_loss: 4.9190 - val_accuracy: 0.6006\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9292 - accuracy: 0.6349 - val_loss: 4.9577 - val_accuracy: 0.6003\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9250 - accuracy: 0.6340 - val_loss: 4.9592 - val_accuracy: 0.5998\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9192 - accuracy: 0.6343 - val_loss: 4.9337 - val_accuracy: 0.5989\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9192 - accuracy: 0.6336 - val_loss: 4.9340 - val_accuracy: 0.5986\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9135 - accuracy: 0.6343 - val_loss: 4.9798 - val_accuracy: 0.5996\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9046 - accuracy: 0.6345 - val_loss: 5.0420 - val_accuracy: 0.6003\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9351 - accuracy: 0.6341 - val_loss: 5.0832 - val_accuracy: 0.6009\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9567 - accuracy: 0.6341 - val_loss: 5.0131 - val_accuracy: 0.5998\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8971 - accuracy: 0.6340 - val_loss: 4.9610 - val_accuracy: 0.5973\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.9277 - accuracy: 0.6335 - val_loss: 4.9894 - val_accuracy: 0.5982\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.8960 - accuracy: 0.6347 - val_loss: 5.0324 - val_accuracy: 0.6002\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.9048 - accuracy: 0.6346 - val_loss: 5.0296 - val_accuracy: 0.6001\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8921 - accuracy: 0.6347 - val_loss: 4.9989 - val_accuracy: 0.5979\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.8966 - accuracy: 0.6348 - val_loss: 4.9872 - val_accuracy: 0.5973\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.8990 - accuracy: 0.6349 - val_loss: 5.0148 - val_accuracy: 0.5995\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8709 - accuracy: 0.6347 - val_loss: 5.0526 - val_accuracy: 0.5999\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.8704 - accuracy: 0.6342 - val_loss: 5.0403 - val_accuracy: 0.5990\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8536 - accuracy: 0.6340 - val_loss: 5.0056 - val_accuracy: 0.5979\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.8593 - accuracy: 0.6346 - val_loss: 5.0017 - val_accuracy: 0.5977\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.8572 - accuracy: 0.6346 - val_loss: 5.0337 - val_accuracy: 0.5985\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.8353 - accuracy: 0.6343 - val_loss: 5.0712 - val_accuracy: 0.5990\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.8300 - accuracy: 0.6349 - val_loss: 5.0813 - val_accuracy: 0.5992\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.8285 - accuracy: 0.6348 - val_loss: 5.0749 - val_accuracy: 0.5986\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8186 - accuracy: 0.6346 - val_loss: 5.0791 - val_accuracy: 0.5988\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8151 - accuracy: 0.6347 - val_loss: 5.1309 - val_accuracy: 0.5995\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8668 - accuracy: 0.6346 - val_loss: 5.2042 - val_accuracy: 0.6006\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.9545 - accuracy: 0.6349 - val_loss: 5.0806 - val_accuracy: 0.5979\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.8170 - accuracy: 0.6348 - val_loss: 5.0177 - val_accuracy: 0.5935\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.8682 - accuracy: 0.6347 - val_loss: 5.1729 - val_accuracy: 0.6001\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8963 - accuracy: 0.6341 - val_loss: 5.0623 - val_accuracy: 0.5942\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.8445 - accuracy: 0.6346 - val_loss: 5.0796 - val_accuracy: 0.5969\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8153 - accuracy: 0.6347 - val_loss: 5.1291 - val_accuracy: 0.5988\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.8201 - accuracy: 0.6346 - val_loss: 5.0761 - val_accuracy: 0.5928\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.8178 - accuracy: 0.6346 - val_loss: 5.1749 - val_accuracy: 0.5995\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.8038 - accuracy: 0.6346 - val_loss: 5.1103 - val_accuracy: 0.5966\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.7764 - accuracy: 0.6345 - val_loss: 5.0859 - val_accuracy: 0.5961\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.7796 - accuracy: 0.6349 - val_loss: 5.1677 - val_accuracy: 0.5989\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.7822 - accuracy: 0.6351 - val_loss: 5.1221 - val_accuracy: 0.5969\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.7640 - accuracy: 0.6343 - val_loss: 5.1328 - val_accuracy: 0.5967\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.7468 - accuracy: 0.6349 - val_loss: 5.1709 - val_accuracy: 0.5993\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.7629 - accuracy: 0.6347 - val_loss: 5.1214 - val_accuracy: 0.5966\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.7446 - accuracy: 0.6344 - val_loss: 5.1422 - val_accuracy: 0.5963\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.7334 - accuracy: 0.6350 - val_loss: 5.1847 - val_accuracy: 0.5985\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.7385 - accuracy: 0.6354 - val_loss: 5.1578 - val_accuracy: 0.5969\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 2.7180 - accuracy: 0.6352 - val_loss: 5.1376 - val_accuracy: 0.5931\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.7466 - accuracy: 0.6352 - val_loss: 5.1804 - val_accuracy: 0.5973\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.7119 - accuracy: 0.6347 - val_loss: 5.2102 - val_accuracy: 0.5983\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.7196 - accuracy: 0.6348 - val_loss: 5.1758 - val_accuracy: 0.5935\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.7097 - accuracy: 0.6348 - val_loss: 5.1757 - val_accuracy: 0.5938\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 2.6991 - accuracy: 0.6351 - val_loss: 5.2098 - val_accuracy: 0.5980\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6976 - accuracy: 0.6354 - val_loss: 5.2167 - val_accuracy: 0.5977\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.6880 - accuracy: 0.6357 - val_loss: 5.2029 - val_accuracy: 0.5938\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.6808 - accuracy: 0.6353 - val_loss: 5.2028 - val_accuracy: 0.5928\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.6920 - accuracy: 0.6352 - val_loss: 5.2098 - val_accuracy: 0.5947\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6720 - accuracy: 0.6356 - val_loss: 5.2358 - val_accuracy: 0.5970\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6799 - accuracy: 0.6351 - val_loss: 5.2440 - val_accuracy: 0.5972\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6757 - accuracy: 0.6343 - val_loss: 5.2314 - val_accuracy: 0.5947\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6605 - accuracy: 0.6350 - val_loss: 5.2215 - val_accuracy: 0.5934\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.6662 - accuracy: 0.6355 - val_loss: 5.2343 - val_accuracy: 0.5945\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6537 - accuracy: 0.6355 - val_loss: 5.2531 - val_accuracy: 0.5950\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6451 - accuracy: 0.6351 - val_loss: 5.2637 - val_accuracy: 0.5948\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.6427 - accuracy: 0.6355 - val_loss: 5.2789 - val_accuracy: 0.5964\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.6609 - accuracy: 0.6355 - val_loss: 5.3079 - val_accuracy: 0.5976\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.7248 - accuracy: 0.6352 - val_loss: 5.2910 - val_accuracy: 0.5966\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.6684 - accuracy: 0.6352 - val_loss: 5.2278 - val_accuracy: 0.5902\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.7018 - accuracy: 0.6359 - val_loss: 5.2543 - val_accuracy: 0.5940\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.6413 - accuracy: 0.6356 - val_loss: 5.2959 - val_accuracy: 0.5969\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6708 - accuracy: 0.6351 - val_loss: 5.2767 - val_accuracy: 0.5945\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6321 - accuracy: 0.6356 - val_loss: 5.2675 - val_accuracy: 0.5902\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.6685 - accuracy: 0.6353 - val_loss: 5.2929 - val_accuracy: 0.5948\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 117ms/step - loss: 2.6218 - accuracy: 0.6356 - val_loss: 5.3143 - val_accuracy: 0.5970\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6419 - accuracy: 0.6349 - val_loss: 5.2909 - val_accuracy: 0.5945\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6073 - accuracy: 0.6354 - val_loss: 5.2709 - val_accuracy: 0.5905\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.6412 - accuracy: 0.6356 - val_loss: 5.2887 - val_accuracy: 0.5932\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6031 - accuracy: 0.6357 - val_loss: 5.3140 - val_accuracy: 0.5964\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6143 - accuracy: 0.6350 - val_loss: 5.3035 - val_accuracy: 0.5944\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.5952 - accuracy: 0.6357 - val_loss: 5.2778 - val_accuracy: 0.5900\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6290 - accuracy: 0.6358 - val_loss: 5.2795 - val_accuracy: 0.5911\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6026 - accuracy: 0.6355 - val_loss: 5.3248 - val_accuracy: 0.5956\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.6106 - accuracy: 0.6352 - val_loss: 5.3377 - val_accuracy: 0.5963\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.6070 - accuracy: 0.6358 - val_loss: 5.3027 - val_accuracy: 0.5911\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.5935 - accuracy: 0.6356 - val_loss: 5.3017 - val_accuracy: 0.5892\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6074 - accuracy: 0.6357 - val_loss: 5.3435 - val_accuracy: 0.5956\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6050 - accuracy: 0.6357 - val_loss: 5.3442 - val_accuracy: 0.5954\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.5795 - accuracy: 0.6349 - val_loss: 5.3075 - val_accuracy: 0.5880\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.6210 - accuracy: 0.6347 - val_loss: 5.3135 - val_accuracy: 0.5900\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.5770 - accuracy: 0.6365 - val_loss: 5.3571 - val_accuracy: 0.5954\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.5834 - accuracy: 0.6354 - val_loss: 5.3535 - val_accuracy: 0.5942\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.5509 - accuracy: 0.6351 - val_loss: 5.3194 - val_accuracy: 0.5881\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.5982 - accuracy: 0.6357 - val_loss: 5.3404 - val_accuracy: 0.5937\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.5430 - accuracy: 0.6359 - val_loss: 5.3638 - val_accuracy: 0.5954\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.5524 - accuracy: 0.6355 - val_loss: 5.3472 - val_accuracy: 0.5925\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.5318 - accuracy: 0.6362 - val_loss: 5.3414 - val_accuracy: 0.5900\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.5524 - accuracy: 0.6356 - val_loss: 5.3658 - val_accuracy: 0.5942\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 2.5181 - accuracy: 0.6355 - val_loss: 5.3742 - val_accuracy: 0.5947\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.5165 - accuracy: 0.6359 - val_loss: 5.3575 - val_accuracy: 0.5924\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.5114 - accuracy: 0.6353 - val_loss: 5.3545 - val_accuracy: 0.5919\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.5055 - accuracy: 0.6353 - val_loss: 5.3762 - val_accuracy: 0.5935\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.4898 - accuracy: 0.6359 - val_loss: 5.3858 - val_accuracy: 0.5932\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.4848 - accuracy: 0.6355 - val_loss: 5.3872 - val_accuracy: 0.5929\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.4773 - accuracy: 0.6362 - val_loss: 5.3963 - val_accuracy: 0.5918\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.4745 - accuracy: 0.6362 - val_loss: 5.3950 - val_accuracy: 0.5916\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 2.4680 - accuracy: 0.6358 - val_loss: 5.3994 - val_accuracy: 0.5921\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.4600 - accuracy: 0.6358 - val_loss: 5.4073 - val_accuracy: 0.5925\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.4523 - accuracy: 0.6355 - val_loss: 5.4067 - val_accuracy: 0.5929\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.4547 - accuracy: 0.6357 - val_loss: 5.4193 - val_accuracy: 0.5942\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.4818 - accuracy: 0.6352 - val_loss: 5.4490 - val_accuracy: 0.5957\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.6350 - accuracy: 0.6324 - val_loss: 5.4090 - val_accuracy: 0.5897\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.5614 - accuracy: 0.6357 - val_loss: 5.4169 - val_accuracy: 0.5768\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.7530 - accuracy: 0.6272 - val_loss: 5.4390 - val_accuracy: 0.5972\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.7025 - accuracy: 0.6350 - val_loss: 5.3640 - val_accuracy: 0.5831\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.7113 - accuracy: 0.6311 - val_loss: 5.4161 - val_accuracy: 0.5989\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.6370 - accuracy: 0.6347 - val_loss: 5.3264 - val_accuracy: 0.5905\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.5761 - accuracy: 0.6353 - val_loss: 5.3649 - val_accuracy: 0.5957\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 2.5477 - accuracy: 0.6341 - val_loss: 5.3539 - val_accuracy: 0.5916\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.5209 - accuracy: 0.6355 - val_loss: 5.3903 - val_accuracy: 0.5970\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.4972 - accuracy: 0.6353 - val_loss: 5.3539 - val_accuracy: 0.5932\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 2.4778 - accuracy: 0.6361 - val_loss: 5.3818 - val_accuracy: 0.5958\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 120ms/step - loss: 2.4795 - accuracy: 0.6351 - val_loss: 5.3678 - val_accuracy: 0.5911\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.4764 - accuracy: 0.6361 - val_loss: 5.4028 - val_accuracy: 0.5972\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.4609 - accuracy: 0.6359 - val_loss: 5.3634 - val_accuracy: 0.5912\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 2.4525 - accuracy: 0.6364 - val_loss: 5.4099 - val_accuracy: 0.5967\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 122ms/step - loss: 2.4496 - accuracy: 0.6356 - val_loss: 5.3888 - val_accuracy: 0.5913\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 119ms/step - loss: 2.4203 - accuracy: 0.6363 - val_loss: 5.4122 - val_accuracy: 0.5948\n",
            "[[1.15968194e-03 7.26084644e-03 7.37322634e-03 ... 3.08074538e-10\n",
            "  2.69923778e-10 6.12387308e-10]\n",
            " [5.61950356e-02 2.70224316e-03 1.25505088e-03 ... 1.52212791e-11\n",
            "  1.86077161e-11 5.14134291e-11]\n",
            " [9.99707878e-01 4.41508791e-06 9.73179340e-07 ... 1.95901081e-18\n",
            "  5.65392218e-18 2.20925764e-17]\n",
            " ...\n",
            " [9.99999881e-01 2.62027400e-09 4.14951434e-10 ... 4.72232568e-26\n",
            "  2.53959692e-25 1.34705819e-24]\n",
            " [9.99999881e-01 1.57944313e-09 2.38742859e-10 ... 9.73786002e-27\n",
            "  5.38977504e-26 2.98373670e-25]\n",
            " [1.00000000e+00 1.07324316e-10 1.08322518e-11 ... 1.32578144e-29\n",
            "  8.83629719e-29 5.85196847e-28]] (22, 7841)\n",
            "[4877, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "ukuphunyuka <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCj9KIUjXuq7"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    }
  ]
}